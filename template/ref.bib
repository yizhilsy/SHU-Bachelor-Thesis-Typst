// 多模态表征之间相关性的评估指标
// idcor
@article{basile2024intrinsic,
  title={Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations},
  author={Basile, Lorenzo and Acevedo, Santiago and Bortolussi, Luca and Anselmi, Fabio and Rodriguez, Alex},
  journal={arXiv preprint arXiv:2406.15812},
  year={2024}
}

// mnist手写数字体识别
@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

// leaky relu
@inproceedings{maas2013rectifier,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  booktitle={Proc. icml},
  volume={30},
  number={1},
  pages={3},
  year={2013},
  organization={Atlanta, GA}
}


// svcca
@inproceedings{svcca,
  author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  volume = {30},
  year = {2017}
}

// cka
@inproceedings{cka,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

// dcor1
@article{dcor1,
author = {G{\'a}bor J. Sz{\'e}kely and Maria L. Rizzo and Nail K. Bakirov},
title = {{Measuring and testing dependence by correlation of distances}},
volume = {35},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2769 -- 2794},
year = {2007},
doi = {10.1214/009053607000000505},
URL = {https://doi.org/10.1214/009053607000000505}
}

// dcor2
@InProceedings{dcor2,
author="Zhen, Xingjian and Meng, Zihang and Chakraborty, Rudrasis and Singh, Vikas",
title="On the Versatile Uses of Partial Distance Correlation in Deep Learning",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="327--346",
isbn="978-3-031-19809-0"
}

// cca 1936
@Inbook{cca,
author="Hotelling, Harold",
title="Relations Between Two Sets of Variates",
bookTitle="Breakthroughs in Statistics: Methodology and Distribution",
year="1992",
publisher="Springer New York",
address="New York, NY",
pages="162--190",
isbn="978-1-4612-4380-9",
doi="10.1007/978-1-4612-4380-9_14",
url="https://doi.org/10.1007/978-1-4612-4380-9_14"
}

// pwcca
@article{pwcca,
  title={Insights on representational similarity in neural networks with canonical correlation},
  author={Morcos, Ari and Raghu, Maithra and Bengio, Samy},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

// twoNN
@article{twoNN,
  title={Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
  author={Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={12140},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

// MLE
@article{mle,
  title={Maximum likelihood estimation of intrinsic dimension},
  author={Levina, Elizaveta and Bickel, Peter},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}


// id在神经网络层中的变化
@inproceedings{NEURIPS2019_cfcce062,
 author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Intrinsic dimension of data representations in deep neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf},
 volume = {32},
 year = {2019}
}

// 归一化互信息
@article{horibe1985entropy,
  title={Entropy and correlation},
  author={Horibe, Yasuichi},
  journal={IEEE transactions on systems, man, and cybernetics},
  number={5},
  pages={641--642},
  year={1985},
  publisher={IEEE}
}

// 多模态互信息
@inproceedings{moschellarelative,
  title={Relative representations enable zero-shot latent space communication},
  author={Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodol{\`a}, Emanuele},
  booktitle={The Eleventh International Conference on Learning Representations}
}

// 多模态潜空间产生可转移的表示
@InProceedings{pmlr-v202-moayeri23a,
  title = 	 {Text-To-Concept (and Back) via Cross-Model Alignment},
  author =       {Moayeri, Mazda and Rezaei, Keivan and Sanjabi, Maziar and Feizi, Soheil},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25037--25060},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v202/moayeri23a.html},
}

// 神经网络相似性测量综述
@article{klabunde2023similarity,
  title={Similarity of neural network models: A survey of functional and representational measures},
  author={Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
  journal={ACM Computing Surveys},
  year={2023},
  publisher={ACM New York, NY}
}


// attention is all you need
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


// 多模态模型的早期工作
@inproceedings{vinyals2015show,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3156--3164},
  year={2015}
}

@misc{Lu2015,
author = {Jiasen Lu and Xiao Lin and Dhruv Batra and Devi Parikh},
title = {Deeper LSTM and normalized CNN Visual Question Answering model},
year = {2015},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/VT-vision-lab/VQA_LSTM_CNN}},
commit = {6c91cb9}
}

@misc{tan2019lxmertlearningcrossmodalityencoder,
      title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers}, 
      author={Hao Tan and Mohit Bansal},
      year={2019},
      eprint={1908.07490},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.07490}, 
}

@inproceedings{lee2018stacked,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={201--216},
  year={2018}
}

@inproceedings{fukui2016mcb,
  title = {Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding},
  author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2016},
  url = {https://arxiv.org/abs/1606.01847}
}

@inproceedings{NEURIPS2019_c74d97b0,
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
  volume = {32},
  year = {2019}
}


// 近期（21年后）多模态大模型
// CLIP
@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

// ALBEF
@misc{li2021alignfusevisionlanguage,
  title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation}, 
  author={Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq Joty and Caiming Xiong and Steven Hoi},
  year={2021},
  eprint={2107.07651},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2107.07651}, 
}

// LLaVA
@inproceedings{llava,
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {34892--34916},
  publisher = {Curran Associates, Inc.},
  title = {Visual Instruction Tuning},
  volume = {36},
  year = {2023}
}

// BLIP-2
@InProceedings{blip2,
  title = 	 {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author =       {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {19730--19742},
  year = 	 {2023},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v202/li23q.html},
}

// Flamingo
@inproceedings{flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\'{n}kowski, Miko\l aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar\'{e}n},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

// InstructBLIP
@misc{instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.06500}, 
}


@inproceedings{nguyen2021do,
title={Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KJNcAkY8tY4}
}



// 多模态数据集及benchmark
// imagenet
@article{imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

// coco2014
@InProceedings{mscoco2014,
author="Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
pages="740--755",
isbn="978-3-319-10602-1"
}

// scienceqa
@inproceedings{scienceqa,
 author = {Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2507--2521},
 publisher = {Curran Associates, Inc.},
 title = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

// vqav2
@InProceedings{vqav2,
author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
title = {Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{n24news,
  title={N24News: A New Dataset for Multimodal News Classification},
  author={Wang, Zhen and Shan, Xu and Zhang, Xiangxie and Yang, Jie},
  booktitle={13th International Conference on Language Resources and Evaluation Conference, LREC 2022},
  pages={6768--6775},
  year={2022},
  organization={European Language Resources Association (ELRA)}
}


// 视觉单模态模型
// ResNet
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

// EfficientNet
@inproceedings{efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

// VIT
@inproceedings{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, G and Gelly, S and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={11975--11986},
  year={2023}
}



// 文本单模态模型
// Bert
@inproceedings{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

// ALBERT
@inproceedings{albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations}
}

// Electra
@inproceedings{electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={International Conference on Learning Representations}
}

// Llama
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{bai2023qwentechnicalreport,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16609}, 
}

@misc{ChatGPT,
	title={ChatGPT},
	author={OpenAI},
	year={2023},
	url={https://openai.com/blog/chatgpt/},
}

@misc{GPT4,
	title={Gpt-4 technical report},
	author={OpenAI},
	year={2023},
}

// mllm综述
@inproceedings{mllmsurvey,
  title={A survey on multimodal large language models for autonomous driving},
  author={Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={958--979},
  year={2024}
}

// 示例参考文献
@misc{liu_survey_2024,
	title = {A {Survey} of {NL2SQL} with {Large} {Language} {Models}: {Where} are we, and where are we going?},
	shorttitle = {A {Survey} of {NL2SQL} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.05109},
	doi = {10.48550/arXiv.2408.05109},
	abstract = {Translating users' natural language queries (NL) into SQL queries (i.e., NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of NL2SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of NL2SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: NL2SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to NL2SQL benchmarks; (3) Evaluation: Evaluating NL2SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to find the root cause and guiding NL2SQL models to evolve. Moreover, we provide a rule of thumb for developing NL2SQL solutions. Finally, we discuss the research challenges and open problems of NL2SQL in the LLMs era.},
	language = {en-US},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Liu, Xinyu and Shen, Shuyu and Li, Boyan and Ma, Peixian and Jiang, Runzhi and Luo, Yuyu and Zhang, Yuxin and Fan, Ju and Li, Guoliang and Tang, Nan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05109 [cs]
TLDR: A comprehensive review of NL2SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: model, data, evaluation, research challenges and open problems of NL2SQL in the LLMs era, and a rule of thumb for developing NL2SQL solutions.},
	keywords = {Computer Science - Databases},
	file = {arXiv Fulltext PDF:/Users/tangzhihao/Zotero/storage/RJZJHCXX/Liu 等 - 2024 - A Survey of NL2SQL with Large Language Models Where are we, and where are we going.pdf:application/pdf;arXiv.org Snapshot:/Users/tangzhihao/Zotero/storage/43RPL8N6/2408.html:text/html},
}


@misc{pourreza_dts-sql_2024,
	title = {{DTS}-{SQL}: {Decomposed} {Text}-to-{SQL} with {Small} {Large} {Language} {Models}},
	shorttitle = {{DTS}-{SQL}},
	url = {http://arxiv.org/abs/2402.01117},
	doi = {10.48550/arXiv.2402.01117},
	abstract = {Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.},
	language = {en-US},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Pourreza, Mohammadreza and Rafiei, Davood},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01117 [cs]
TLDR: A novel two-stage fine-tuning approach is introduced that decomposes the text-to-SQL task into two simpler tasks and improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/tangzhihao/Zotero/storage/4Z7LYHWA/Pourreza和Rafiei - 2024 - DTS-SQL Decomposed Text-to-SQL with Small Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/tangzhihao/Zotero/storage/22LH2UT6/2402.html:text/html},
}