#import "../lib.typ": documentclass, algox, tablex, citex, imagex, subimagex

#let (
  info,
  doc,
  cover,
  declare,
  appendix,
  outline,
  mainmatter,
  conclusion,
  abstract,
  bib,
  acknowledgement,
  under-cover,
) = documentclass(
  info: (
    title: "多模态表征的非线性相关性分析研究",
    school: "计算机工程与科学学院",
    major: "计算机科学与技术",
    student_id: "21121889",
    name: "陆诗雨",
    supervisor: "王含章",
    date: "2025.2.17----2025.5.30",
  ),
)

#show: doc.with(
  fallback: false // 为true时字体缺失时使用系统默认，不显示豆腐块
)
#cover()
#declare()

#abstract(
  keywords: ("多模态模型", "非线形相关性", "模态融合", "评估模态对齐"),
  keywords-en: ("MultiModal Model", "Nolinear Correlation", "Modality Interface", "Eval Representation Alignment"),
)[
  近年来以大型语言模型为中心的多模态模型取得了显著的进展，这些多模态模型在训练时往往会冻结部分或全部的各模态模型的参数，并专注于模态融合网络的训练及设计。模态融合网络的作用是将不同模态的表征对齐到同一个语义空间中，以便大型语言模型可以更好地理解不同模态间的语义关系。然而，现有的研究方法往往通过经验的方式或是设计相关的预训练任务来粗略地考虑模态对齐，缺乏一个客观的易于计算的评估算法来反映模型在训练阶段时不同时刻模态对齐的质量，从而指导模态对齐融合网络的训练，设计并以此提升模态对齐的质量。为了解决这个问题，本文从衡量多模态表征之间的非线形相关性出发，选取了多种衡量表征之间相关性的指标，并设计了多种实验条件来详实地对比这些指标的性能。在所有的实验条件中内在维度相关性 $I_d\C\o\r$ 指标都表现出了最好的性能，尤其是在捕获多模态表征的非线形相关性时。因此本文基于 $I_d\C\o\r$ 开发了一个可以用于评估多模态大模型模态对齐质量的计算高效的评估算法evalmm，并在主流的多模态大模型上进行了实验验证其合理性。本文利用evalmm算法对多模态大型语言模型模态对齐程度对下游任务的影响进行了探讨，实验结果表明模态对齐程度与下游任务性能之间存在一定的正相关性。

][
  In recent years, multimodal models centered around large language models (LLMs) have made remarkable progress. During training, these models often freeze part or all of the modality models and focus on the design and training of the modality fusion network. The primary role of the fusion network is to align representations from different modalities into a shared semantic space, enabling the LLM to better understand the semantic relationships across modalities. However, existing approaches typically rely on empirical designs or auxiliary pretraining tasks to achieve coarse modality alignment. These methods lack an objective and computationally efficient evaluation algorithm to reflect the quality of modality alignment at different stages of training. Such an evaluation mechanism would be valuable in guiding the training and design of the fusion network and ultimately improving alignment quality. To address this issue, this paper investigates the nonlinear correlations between multimodal representations by selecting various metrics for measuring representations' correlation. This paper design multiple experimental conditions to comprehensively compare the performance of these metrics.Among all experimental conditions, the Intrinsic Dimension Correlation $I_d\C\o\r$ metric consistently demonstrated the best performance, particularly in capturing the nonlinear correlations between multimodal representations. Based on this finding, this paper developed a computationally efficient evaluation algorithm, evalmm, grounded in the $I_d\C\o\r$ metric, to assess the modality alignment quality in large-scale multimodal models. This paper conducted experiments on mainstream multimodal large model to validate the effectiveness and reliability of evalmm. Furthermore, this paper explores the impact of modality alignment quality—measured by evalmm—on downstream task performance. Experimental results indicate a certain positive correlation between the degree of modality alignment and the performance on downstream tasks.
]

#outline()

#show: mainmatter
= 绪论

== 研究背景及意义
在人类社会及科技发展的过程中，对于多个模态信息的呈现、处理和理解需求一直存在。早期人们希望可以实现多媒体传播来传递例如视频、图像、文本等信息，这促进了互联网技术和多媒体技术的发展。随着近年来人工智能技术的快速发展，人们希望构建一种通用的智能体来处理并帮助理解多个模态之间的信息，例如根据输入的描述从视频中提取关键帧，根据上传的图像及问题生成相应的回答，亦或是根据描述快速地从海量数据中检索出感兴趣的图像等。这促进了人们对于多模态研究的关注程度，多模态研究的重要程度在于提供了一种高效准确的多信息处理方式以及构建通用人工智能的能力，这也是人工智能发展的核心愿景。

早期研究者设计的多模态模型主要适用于特定领域的任务，例如图文检索，图片字幕描述，视觉语言问答等任务，以这些任务为代表涌现了一批优秀的多模态模型，例如Stacked Cross Attention @lee2018stacked，Neural Image Caption @vinyals2015show，Multimodal Compact Bilinear Pooling @fukui2016mcb 等模型。然而这些模型在设计和训练时往往引入了特定任务的限定，使得模型的能力仅能专注于特定的任务，对于其他的任务适应能力差。这使得多模态领域的技术局限在有限的应用场景中，并催发人们致力于构建一个统一的泛化能力强的多模态模型的强烈兴趣。

近年来随着算力的提升和数据集数量的增加，模型朝着大规模的方向发展。通过扩展模型参数量和训练数据量，在大型语言模型（LLM）和大型视觉模型（LVM）领域有了显著的进展。大型语言模型在文本生成（Text Generation），文本理解（In-Context Learning），指令跟随(Instruct Following)等任务上的能力呈现出了优异的水平；大型视觉模型在目标检测（Object Detection），图像分割（Image Segmentation），图像分类（Image Classification）等任务上也表现出了令人印象深刻的性能。随着大模型在各自模态中的成功，研究者们开始关注在多模态领域中的进一步研究，多模态模型的正式定义是指具有同时处理和理解多种模态（如文本、图像等模态）的信息的能力，并且进一步可以完成多模态任务如图文检索（Iamge-Text Retrieval），视觉问答（Visual Qusetion Answer）的模型。大型语言模型在指令跟随及文本理解上的强大性能显示了文本作为通用任务指令的接口的可行性，借助预训练大型语言模型出色的零样本及少样本能力，可以帮助模型更好地建模不同模态间的语义关联，因此研究者们开始关注以大型语言模型为基础的多模态模型的研究。一个直接且作为当前主流的思路是应用在各自模态中取得显著性能的大模型，然而，如何有效地将不同模态的信息进行融合并且让多模态模型准确地理解来自不同模态的信息来完成多模态任务是多模态领域中的关键问题。

研究者们在近年来提出了多种基于大型语言模型的多模态模型以及相关的多模态融合方法。值得注意的是，无论是在设计多模态模型的网络还是在设计多模态模型的预训练任务上，研究者们都强调了多模态对齐的重要性。在ALBEF @li2021alignfusevisionlanguage 的工作中，研究者从理论分析出发，借助多模态互信息下限最大化的角度揭示了模态对齐的价值和重要性。同样的，ALBEF在网络设计上也引入了图文对比学习的网络来实现模态对齐的目标。类似的，在LLaVA @llava 的工作中，研究者将训练步骤拆分为了两个阶段，第一阶段在大量的图文描述对上进行训练，目标也是模态对齐。

然而，虽然研究者们都肯定了多模态大模型模态对齐的价值，但现有的研究未针对模态对齐融合的质量进行详细的研究，研究者们往往通过经验的方式或是设计相关的预训练任务（如图文对比学习任务等）来粗略地考虑是否对齐，缺乏一个客观的易于计算的评估指标来反映模型在训练阶段时不同时刻模态对齐融合的质量，从而指导模态对齐融合网络的训练，设计并以此提升模态对齐融合的质量和多模态大语言模型在下游任务上的性能。

在深度神经网络的可解释性领域，为了衡量两个神经网络是否相似，研究者们借助计算由两个神经网络产生的不同数据潜在表征之间的统计相关性来衡量相似性 @nguyen2021do 。本文受到这种方法的启发，通过计算不同模态的数据（主要是图像-文本对的数据）在多模态大语言模型中产生的数据表征间的统计相关性来衡量多模态大语言模型的模态对齐程度（能力）。然而这些多模态表征在如今的多模态大模型中不仅是高维的，且互相存在复杂的非线形关系，利用传统的线形相关性及非线形相关性计算方法如CCA @cca ， SVCCA @svcca ， CKA @cka ，dCor @dcor1 @dcor2 等都不能准确有效地衡量相关性。幸运的是，最近在 $I_d\C\o\r$ @basile2024intrinsic 的工作中提出的 $I_d\C\o\r$ 计算指标在衡量表征之间的相关性上表现出了远超传统方法的性能， 尤其是在多模态表征领域。它利用了归一化互信息，但使用了表征的本征维度来代替表征的熵，使用低维数据集中可以恢复的在高维数据集中自变量的比例衡量了相关性。由于 $I_d\C\o\r$ 衡量指标在多模态表征上的出色表现，本文基于 $I_d\C\o\r$ 指标，提出了可以用于评估多模态大模型的模态对齐质量的评估算法evalmm，且提出了可适用于庞大评估模态对齐数据集的分组计算期望算法。并在近年来主流的开源多模态大模型上进行了实验验证证明其合理性。

本文希望本文提出的多模态表征的非线性相关性评估算法可以帮助更准确严谨地衡量多模态大模型模态对齐的性能。此外本文也注意到多模态表征的非线性相关性指标可以进一步帮助模态对齐融合网络的设计与训练过程，并有助于多模态大语言模型在下游任务上微调后的性能；同时多模态表征的非线性相关性指标还有可能有助于构建高质量多模态数据集。本文也针对多模态表征的非线形相关性指标可能带来的应用进行了比较详实的实验探索。本文的研究以期进一步推动多模态大语言模型的发展。

== 相关研究现状

=== 多模态模型的模态融合方式

在多模态领域的研究历史中，研究者们倾向于首先为每个模态选取一个表现优异的单模态模型，这些单模态模型往往是预训练好的，而将研究的主要工作放在设计不同种模态表征之间的融合方式上。多模态领域的早期工作中，融合不同模态的表征采取了简单直接的方式 @vinyals2015show @Lu2015。以视觉-语言的多模态模型为例，在Neural Image Caption @vinyals2015show 的工作中，图像表征由图像在卷积神经网络（CNN）的最后一层隐藏层得到，并直接送入循环神经网络（RNN）的decoder中进行图像描述生成；在Deeper LSTM and normalized CNN @Lu2015 的工作中，对由卷积神经网络得到的图像嵌入向量和由长短期记忆网络（LSTM）得到的问题文本的嵌入向量进行逐点相乘，得到一个新的向量作为图像和文本的融合表征并在之后送入多层分类器去预测词语的概率分布。简单直接的模态表征融合无法捕捉不同模态表征间深层次的语义关系，且受限于早期的单模态模型的性能，这些方法的性能有限，同时模型往往是为了特定的任务而设计并训练的，缺乏泛化到多种任务上的能力。

之后的研究随着新的优秀的单模态模型的提出，尤其是在目标检测领域及自然语言处理领域取得成功的模型，同时随着注意力机制 @vaswani2017attention 及多模态领域中交叉注意力机制理论@NEURIPS2019_c74d97b0 的成熟与推广，研究者们基于新的单模态模型提出了更为复杂的基于注意力机制的模态融合方法。例如在LXMERT @tan2019lxmertlearningcrossmodalityencoder 的工作中，图像模态的表征由目标检测模型检测到的物体的区域感兴趣特征及位置信息特征得到，并在之后与文本模态的表征进行交叉注意力机制的融合。然而，这种模态融合方法并没有将不同模态的表征映射进同一个维度空间中进行融合，这对于模型捕获到不同模态表征间的语义关联造成了阻碍；同时目标检测的视觉模型在预训练和推理时的计算资源消耗较大 @li2021alignfusevisionlanguage。

近年来大型语言模型在自然语言处理领域中取得了一系列成功，如：ChatGPT @ChatGPT 和GPT-4 @GPT4 等模型，大型语言模型为研究者展现了其卓越的指令跟随及文本理解能力。得益于预训练大型语言模型的开源，如LLaMA @touvron2023llamaopenefficientfoundation
 , Vicuna @vicuna2023 , Qwen @bai2023qwentechnicalreport 等，使得在多模态研究中构建以大型语言模型为中心的多模态模型成为可能。这是因为可以利用文本作为通用任务指令的接口得以让模型可以训练并理解多种不同的复杂任务，同时借助预训练大型语言模型出色的语言理解和指令跟随能力，这使得构建通用的可以泛化到多种不同多模态任务的模型变成了可能。目前，以大模型为基础的多模态模型主要有三种模态融合的方式：分别是以LLaVA @llava 为代表的直接投影法，以BLIP-2 @blip2 为代表的基于Attention架构的Q-Former方法，以及以Flamingo @flamingo 为代表的在大型语言模型内部插入额外参数模块以实现文本特征和视觉特征的交互融合，从而将视觉信息注入到文本的生成过程中的特征极板块融合法。这三种方法的作用机理如@img:mllm 所示。

从多模态模型的发展历程中可以看出，多模态模型的研究重点主要在于模态融合的方式上。值得提出的是，近年来以大型预训练语言模型为中心的多模态大模型的研究倾向于冻结部分或全部视觉模型和文本模型的参数，专注于训练模态融合的网络参数；同时在大型语言模型的指令微调技术也应用到了多模态大模型当中 @llava @instructblip 。模态融合方式直接影响了多模态模型能否建模不同模态间复杂的语义，进一步呼吁了设计合理高效的模态融合网络及评估算法的需求。


#figure(
  image(
    "figures/mllm.png",
    width: 90%,
  ),
  kind: "image",
  supplement: [图],
  caption: [多模态大型语言模型常见模态融合方式示意图@mllmsurvey], // 英文图例
)<mllm>

#v(1.5em)


// #figure(
//   image(
//     "figures/mllmdevelopment.png",
//     width: 100%,
//   ),
//   kind: "image",
//   supplement: [图],
//   caption: [多模态大型语言模型发展示意图@mllmsurvey], // 英文图例
// )<mllmdevelopment>

// #v(1.5em)


=== 评估多模态对齐程度方法
近年来的研究中，研究者们从多种角度出发提出了评估多模态模型模态对齐程度的方法。对表征集合中的数据点进行降维并绘制降维后在二维或三维空间中的数据点散点图是一种常用的可视化角度的评估多模态对齐的方法。

1. PCA方法
PCA是一种经典的线性降维方法，主要通过正交变换将原始数据映射到一组新的、互相独立的主成分上。这些主成分按数据方差大小排序，前几个主成分保留了数据中大部分的信息。PCA适用于高维数据的可视化、噪声过滤和特征压缩，计算效率高，具有良好的可解释性。

2. t-SNE方法
t-SNE是一种非线性降维技术，特别适合高维数据的可视化。它通过最小化高维空间和低维空间中点对之间的概率分布差异，有效保留了局部结构，使得相似的数据点在低维空间中聚集在一起。虽然t-SNE在保持局部结构方面表现优异，但其计算复杂度较高，且无法直接泛化到新数据。

3. UMAP方法
UMAP是一种基于流形学习和图论的非线性降维方法，既能很好地保持数据的局部结构，也能较好地保留全局结构。相比t-SNE，UMAP计算更高效，并支持有监督学习和新样本的嵌入。它在保持聚类结构和揭示数据的整体形态方面表现出色，常用于可视化和预处理。

上述的三种降维方法都可以用于对多模态表征进行降维并可视化不同模态表征降维后数据点的分布，但无法准确给出多模态表征之间具体的相关性系数值。无法进行进一步的应用。这也是本文的研究动机之一，本文希望可以寻找到一个客观合理的评估多模态表征相关性的指标来衡量多模态表征之间的相关性，并且可以在多模态大模型模态对齐的评估中进行高效的计算。

=== 潜在空间表征相关性衡量指标
在神经网络的可解释性领域，一个研究的关键点是试图判断两个不同的神经网络是否可以在相同的数据集上学习到相似的方式去处理数据。虽然定义两个神经网络具有相似的行为存在一定的困难和歧义，但在最近几年研究者们从神经网络学习到的表征之间的统计相关性入手，取得了重大的进展。基于表征学习和统计相关性，研究者们已经提出了许多线形及非线形的可用于衡量表征间相关性的指标，如Singular Value Canonical Correlation Analysis (SVCCA) @svcca ，Centered Kernel Alignment （CKA） @cka，Distance Correlation（dCor）@dcor1 @dcor2，Canonical Correlation Analysis（CCA） @cca 等。这些技术已被广泛用于更深入地了解神经模型处理信息方式的各个方面。

然而，上述指标存在一些局限性。例如Canonical Correlation Analysis（CCA）主要从线形相关的角度出发去衡量表征之间的相关性，但在实际情况下，不同的表征之间往往存在复杂的非线形关系；从表征的数据点之间的距离出发的Distance Correlation（dCor）指标虽然能够捕获一定的非线形关系，但是在高维的表征上表现不佳 @basile2024intrinsic；从硬件要求的角度出发，Centered Kernel Alignment （CKA）在计算大规模数据集上得到的表征时需要消耗过高的显存及计算资源，且计算复杂度较高。在如今的多模态模型朝着大规模，使用大型语言模型的趋势下，现有的表征相关性指标在衡量多模态大模型产生的大量高维，彼此间由不同模态的模型产生的表征时的性能不甚理想。

在最近的研究中，Lorenzo @basile2024intrinsic 等人提出了一种新的衡量表征之间的相关性的方法，称为Intrinsic Distance Correlation（idcor）。该方法使用了归一化互信息，但使用了表征的本征维度来代替表征的熵，并使用低维数据集中可以恢复的在高维数据集中自变量的比例来衡量相关性。该指标在高维非线形的表征上相比之前的几种指标表现出了更好的性能，此外在多模态数据集上也具有强大的能力去衡量不同模态的模型产生的表征之间的相关性（例如图文模型在MSCOCO @mscoco2014 数据集上产生的表征之间的相关性），相比原先的基线指标dCor具有大幅的性能领先。同时，idcor指标的计算代价主要来自计算表征集合的本征维度。而运用TwoNN @twoNN 来计算本征维度已被大量的工作证实具有高效性 @NEURIPS2019_cfcce062 ，且TwoNN在大规模数据上的表征点的高维空间密度高度不均匀时也能表现出良好的性能。因此idcor指标的计算资源代价也是优秀的。

然而，在Intrinsic Distance Correlation（IdCor） 的工作中却尚未将此指标应用于近年来主流的开源多模态大模型产生的不同模态的表征评估上，仅针对一些早期的视觉及文本模态模型如ResNet-18 @resnet ，EfficientNet @efficientnet ， VIT @vit ，Bert @bert ，ALBERT @albert ，Electra @electra 及CLIP @clip 下属的单模态模型上进行了多模态表征相关性的实验 @basile2024intrinsic 。虽然这些单模态模型常被用作构建多模态大模型的模态编码器，但在如今多模态大模型火热发展的趋势下所造成的贡献较为局限。这也是本文的研究动机之一。本文希望借助idcor指标的优越性能，设计出一个可以用于评估多模态大模型模态对齐质量的高效评估算法，并在主流的多模态大模型上进行实验验证。

=== 多模态潜在空间对齐的理论研究现状
在Luca Moschella @moschellarelative 等人的工作中经验性地揭示了如果多模态的数据存在某种语义对齐（例如图像及其对应的描述），那么就可以在潜在空间中传递知识。这种知识传递的可行性由原始表征的每个点与一组固定的锚点之间的距离所代表的相对表征来实现。Luca Moschella等人还表明使用这种相对表征，可以不需要额外训练的情况下，将来自不同模态的模型的编码器和解码器拼接在一起 @basile2024intrinsic。

同时也有研究表明 @pmlr-v202-moayeri23a ，当对对齐数据（即相同的数据或共享某些语义的多模态数据，例如图像-标题对）进行评估时，大型最先进的视觉和文本编码器可以产生可转移的表示。事实上，一个简单的线性变换通常就足以将一个潜在空间映射到另一个潜在空间。从而实现借助另一个模态的表征来完成对应的下游任务例如分类等。

在语义对齐的多模态数据上存在的多模态潜在空间的相关性可以用于建模不同模态间深层次的语义关系。如果存在一个不依赖任何下游任务评估的情况下检测这些相关性的指标，那么可以用于帮助研究者衡量模型模态对齐程度，以便及时做出调整更好的让模型建模不同模态之间的语义。


== 本课题的研究难点
1. 研究难点一：在多模态大语言模型中的多模态表征往往具有高维，强非线形等特点，如何寻找一个客观合理的指标衡量多模态表征的相关性将是一个难点。同时，高维往往代表着计算复杂度的庞大，如何有效控制时间复杂度及计算资源的消耗也是一个必须考虑的难点。

2. 研究难点二：寻找到一个客观合理的指标后，如何在评估模态对齐算法的设计中有效利用该指标来客观合理地评估模态对齐，并设计相关的实验验证所设计的评估算法的合理，这将是一个可能的难点。

3. 研究难点三：在探讨多模态大语言模型的模态对齐程度与微调后在下游任务上的性能之间的关系时，需要涉及多模态大语言模型的训练。多模态大语言模型的训练涉及多个阶段且不同的阶段会更新不同模态模型的参数，这导致了复杂的训练流程。其中准确理解其训练流程并编写高效的训练代码将会是一个可能的难点。


== 本文研究内容
在多模态领域的研究中，设计一个高效的模态融合网络一直是一个重要的研究方向。在很多研究者的工作中，如Junnan Li @li2021alignfusevisionlanguage @blip2， Haotian Liu @llava 等人，都从模态融合网络的设计或是多模态预训练任务的角度上出发来保证模态融合网络的模态对齐性能。在一些研究中，如Haotian Liu @llava 等人通过消融的实验方式证实了缺乏模态对齐的训练步骤会对多模态模型的性能造成显著的下降。尽管现有的研究都在关注模态对齐的重要性，但现有的研究往往缺乏一个客观的评估指标来衡量模态对齐的质量亦或是观察多模态模型在训练过程中模态对齐程度的变化。研究者们同样也只能从多模态模型在下游任务上的性能来间接地推测设计的模态融合网络是否合理。并不利于多模态模型的进一步设计与优化。

针对以上提到的这些问题，本文的研究内容如下：
// 1. 研究内容一：介绍现有的主流多模态大语言模型的设计架构（如BLIP-2 @blip2，LLaVA @llava，Flamingo @flamingo 等），理解分析主流架构在模型设计尤其是多模态表征对齐融合网络设计上的思路并捋清发展历程，分析各种多模态表征对齐融合网络的优劣和特点。

1. 研究内容一：研究多模态表征的非线性相关性，找出一个客观准确的，计算高效的评估相关性指标，并通过多角度的实验，如在高维、复杂非线形数据集、视觉模态数据集、现实多模态数据集上与其他计算多模态表征的相关性的指标进行对比，分析其优劣，最终得到一个最佳的计算多模态表征之间的非线性相关性的指标。

3. 研究内容二：基于研究内容一中得到的评估指标，考虑到多模态大型语言模型的实际的模型架构及模态融合网络的实现，设计一个高效的评估算法来衡量多模态大语言模型的模态对齐程度。并在主流的多模态大语言模型上进行实验验证所设计的评估算法的合理性。

3. 研究内容三：借助研究内容二得到的评估算法衡量多模态大语言模型在训练过程中模态对齐程度的变化。并设计实验研究模态对齐融合的程度将如何影响模型在后续下游任务上的性能。下游任务上的性能将借助现有的研究广泛采用的benchmark基准测试：ScienceQA @scienceqa 来进行评估。

== 本文组织架构
本文的第一章是绪论，主要介绍了多模态表征非线形相关性的研究意义及背景，并对相关研究现状进行了简述，并介绍了本文的研究内容和预计的研究难点。本文的第二章对于必备的多模态相关技术进行介绍，包括Transformer、多模态对比学习、近年来的多模态大型语言模型、常见多模态Benchmark等。本文的第三章介绍了非线性相关性评估指标 $I_d\C\o\r$ 的设计思路及实现细节，并在多种实验条件下与基线指标进行了对比实验，验证了 $I_d\C\o\r$ 指标的卓越性能。本文的第四章介绍了基于IdCor指标设计的多模态大语言模型模态对齐评估算法的设计思路及实现细节，并在主流的多模态大语言模型上进行了实验验证，验证了其合理性。并基于此评估算法探讨了预训练阶段模态对齐程度与下游任务性能之间的关系。本文的第五章是结论与展望，对本文的工作进行总结，并对未来可能的研究方向进行展望。



= 多模态相关技术介绍
本章的目的是进行多模态领域的重要技术的简要介绍。本章的前半部分将主要介绍如Transformer、多模态对比学习、多模态融合网络等技术，便于后续章节如 $I_d\C\o\r$ 指标的分析提供背景及理论依据；本章的后半部分将简要介绍多模态领域中所常用的一些开源数据集及benchmark评估任务，为后续章节实验设计提供背景介绍。

== 深度学习与神经网络
深度学习（Deep Learning）是机器学习领域的一个重要分支，其核心是通过多层非线性变换自动学习数据的高层次特征表示。深度学习模型通常以人工神经网络为基础，利用层级结构模拟人脑处理信息的方式，从原始数据中提取复杂的抽象特征。人工神经网络（Artificial Neural Network, ANN）由多个神经元（neurons）按层级方式连接构成，基本结构包括输入层、隐藏层和输出层。深度神经网络（Deep Neural Network, DNN）通过堆叠多个隐藏层，能够建模复杂的非线性函数映射，具有强大的表达能力。训练过程中通过反向传播（Backpropagation）算法结合梯度下降方法不断优化网络参数。近年来，随着大规模数据集的可用性和计算能力的提升，深度学习在计算机视觉、自然语言处理、语音识别以及多模态融合等领域取得了突破性进展，成为人工智能的核心技术之一。

== Transformer
Transformer@vaswani2017attention 是一种基于注意力机制（Attention Mechanism）的深度神经网络架构，模型架构如@img:transformers 所示。Transformer由Vaswani等人于2017年提出，模型的输入是对词序列向量化后的嵌入向量，其核心思想是通过自注意力（Self-Attention）机制建模序列中各个位置之间的依赖关系。与传统的循环神经网络（RNN）相比，Transformer完全摒弃了循环结构，具备更强的并行计算能力和更高效的长期依赖建模能力。Transformer结构主要由编码器（Encoder）和解码器（Decoder）组成，每个模块由多层堆叠的子层构成，包括多头自注意力机制（Multi-Head Self-Attention）、前馈神经网络（Feed-Forward Network）以及层归一化（Layer Normalization）和残差连接@resnet（Residual Connection）。其中，多头注意力机制通过多个注意力头捕捉不同子空间的特征信息，增强模型表达能力。Transformer模型及其思想活跃于自然语言处理（Natural Language Processing）与计算机视觉（Computer Vision）领域，许多基于Transformer架构的优秀性能模型被陆续提出，如BERT@bert，ViT@vit ，大型语言模型（LLM）等，推动了各类任务性能的显著提升。


=== 多头注意力机制
注意力机制的核心思想是模仿人类在处理信息时的注意力分配方式，例如人眼在观察图像时会有所侧重的关注某些区域。具体而言，注意力机制定义了三个矩阵：查询矩阵 $Q^(n times d_k)$ 、键矩阵 $K^(m times d_k)$、值矩阵 $V^(m times d_v)$，首先通过矩阵乘法 $Q times K^T$ 计算查询矩阵中每行查询向量与键矩阵中所有的键向量之间的余弦相似度，称结果为注意力权重矩阵 $W^(n times m)$，并适当引入一个常量对矩阵 $W$ 的所有值进行缩放，再通过Softmax函数对 $W$ 的每一行进行归一化处理，得到最终的权重矩阵 $W$。之后再通过矩阵乘法 $W times V$ 将每行查询向量对键向量的权重应用到值向量的线形组合上，得到最终的输出矩阵 $O^(n times d_v)$ 。具体的计算过程如@eqt:attentioncal 所示。

$
  "Attention"(Q, K, V) = "softmax"(Q K^T / sqrt(d_k))V
$<attentioncal>

Transformer在注意力机制的基础上引入了多头注意力机制，其思想是针对 $Q^(n times d_k)，K^(m times d_k)，V^(m times d_v)$ 矩阵进行 $h$ 次的投影，将初始的矩阵映射到 $h$ 个不同的子空间中，投影到不同子空间的参数是可以学习的，投影后的向量维度为 $d_k / h, d_v / h$，将这些投影后的向量各自进行注意力计算，最后将 $h$ 个子空间中的结果拼接起来。多头注意力机制的计算过程如@eqt:mhattentioncal 和@img:mhattention 所示。在整个多头注意力的计算过程中，不同子空间的注意力计算可以并行进行。

$
  "MultiHead"(Q, K, V) = "Concat"("head"_1,...,"head"_h) \
  "where" "head"_i = "Attention"(Q W_i^Q, K W_i^K, V W_i^V) \
  W_i^Q in RR^(d_k times d_k / h) quad W_i^K in RR^(d_k times d_k / h) quad W_i^V in RR^(d_v times d_v / h)
$<mhattentioncal>

#figure(
  image(
    "figures/multihead-attention.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [Transformer多头注意力机制示意图], // 英文图例
)<mhattention>

#v(1.5em)

=== 位置编码
由于Transformer模型中应用的注意力机制本身是无序的，因此其不具备处理序列数据的顺序信息的能力，因此需要引入位置编码（Positional Encoding）来为输入序列中的每个元素添加位置信息。位置编码采用了正弦和余弦函数，具体计算方式如@eqt:poscal 所示，其中 $\p\o\s$ 和 $i$ 分别表示词所在位置和词向量的维数。将位置编码与词向量相加可以使得嵌入向量包含位置信息，同时可以保证不至于破坏原有的嵌入向量所表示的语义信息。 

$
  \P\E_(\p\o\s, 2i) = sin(\p\o\s \/ 10000^(2i \/ d_k )) \
  \P\E_(\p\o\s, 2i+1) = cos(\p\o\s \/ 10000^(2i \/ d_k ))
$<poscal>


=== 编码器-解码器架构
Transformer模型的架构由编码器和解码器组成，@img:transformers 左侧显示的是编码器，右侧显示的是解码器。其中编码器的作用是理解输入序列的各种信息，解码器的作用是进行输出序列的生成。Transformer的编码器由 $N$ 层网络构成，每层网络都包含有两层子模块：自注意力子模块与前馈神经网络子模块，每个子模块都作用了层归一化与残差连接@resnet 。自注意力子模块的 $Q, K, V$ 矩阵都赋值为上一层编码器网络的输出（初始是经过位置编码后的嵌入向量），从而进行自注意力的计算；前馈神经网络子模块是一个单隐藏层的MLP神经网络，并作用了非线形激活函数，对输入表示进行更高阶的抽象和变换，使模型能够表示更复杂的模式。Transformer的解码器亦由N层网络组成，每层网络包含有三层子模块：掩码自注意力子模块、编码器-解码器注意力子模块与前馈神经网络模块，同样的，每个子模块都作用了层归一化与残差连接。每一层解码器层的输入是生成语句作用位置编码后的嵌入向量，经过掩码自注意力子模块的计算后得到 $Q$ 矩阵输入至编码器-解码器注意力子模块，该子模块的 $K, V$ 矩阵是编码器的输出，经过编码器-解码器注意力子模块的计算后得到的输出作为前馈神经网络子模块的输入，最终得到解码器的输出，此输出即是下一层解码器网络的输入。经过 $N$ 层解码器网络的计算后，并经过线形层的变换和Softmax函数的作用，Transformer模型最终会生成推理得到的序列。

#figure(
  image(
    "figures/transformer.png",
    width: 70%,
  ),
  kind: "image",
  supplement: [图],
  caption: [Transformer模型示意图], // 英文图例
)<transformers>

#v(1.5em)


== 多模态对比学习
多模态对比学习（Multimodal Contrastive Learning）是一种通过拉近语义相关模态样本距离、推远无关模态样本距离的方式，学习跨模态对齐的表征学习方法。OpenAI提出的CLIP（Contrastive Language-Image Pretraining）@clip 模型是该方向的代表性工作，广泛用于图文表征对齐与下游多模态任务。CLIP模型的示意图如@img:clip_contrastive 所示。CLIP模型由一个图像编码器和一个文本编码器组成，分别将图像和文本编码器产生的特征向量映射为同一维度的嵌入向量。在训练阶段，CLIP在大规模图文对数据集上进行对比学习，通过最大化真实图文对在嵌入空间中的相似度，并最小化非匹配图文对之间的相似度，优化一个对比损失函数（如InfoNCE）。具体而言，假设一批图像-文本对有 $N$ 个，将各个模态编码器产生的 $N$ 对图文特征向量进行归一化后，映射进同一维度中，记映射后的图文特征向量分别为 $hat(z)_i^I, hat(z)_i^T, i in [1,N]$ 。之后计算每个图像和每个文本之间的余弦相似度，并乘以一个可学习的温度参数 $tau$，记 $S_(i j)$为第 $i$ 个图像与第 $j$ 个文本之间的相似度。计算公式如@eqt:similaritycal 所示。这 $N$ 对图像-文本对的最终损失是图像与文本两个方向的InfoNCE损失函数的平均值，即图像作为查询时匹配到正确的文本与文本作为查询时匹配到正确的图像，其计算公式如@eqt:infoncecal 所示。CLIP模型的训练基于优化该损失函数。


$
  S_(i j) = (hat(z)_i^I dot hat(z)_i^T) / tau
$<similaritycal>

$
  pound = 1 / (2N) sum_(i=1)^N [-log exp(S_(i i)) / (sum_(j=1)^(N) exp(S_(i j)) ) log exp(S_(i i)) / (sum_(j=1)^(N) exp(S_(j i)) )   ]
$<infoncecal>



多模态对比学习的核心目标是使语义相关的图像与文本在共享嵌入空间中靠得更近，语义不相关的图像与文本在共享嵌入空间中尽量靠得更远。从而实现跨模态的语义对齐。CLIP的对比学习框架不依赖于传统的监督分类标签，而是利用天然配对的图文数据进行预训练，使其在众多下游任务中具有良好的零样本迁移能力，推动了多模态表示学习的快速发展。


#figure(
  image(
    "figures/clip_contrastive.png",
    width: 90%,
  ),
  kind: "image",
  supplement: [图],
  caption: [多模态对比学习（以CLIP为例）], // 英文图例
)<clip_contrastive>

#v(1.5em)

== 开源大型语言模型
=== LLaMA
LLaMA采用标准的Transformer解码器结构，但在模型优化方面做了多项改进，包括使用更高效的分词器（SentencePiece with Byte-Pair Encoding）、更小的嵌入维度扩展比、以及预归一化（Pre-Normalization）等技术，从而提高训练稳定性与收敛速度@touvron2023llamaopenefficientfoundation 。LLaMA系列模型覆盖多个规模，从7B到65B参数量不等，适用于不同算力需求的研究任务。LLaMA模型在多项语言理解和生成基准任务上表现优异，具备良好的语言建模能力、上下文理解能力和生成质量。由于其开源特性，LLaMA也成为众多开源多模态指令微调模型（如LLaVA等@llava）的基础架构，对推动以大型语言模型为中心的多模态研究起到了积极作用。


=== Qwen
Qwen模型基于标准的Transformer解码器架构，融合了多项现代大语言模型的优化技术，具备高效、可扩展的特点。其采用改进版的旋转位置编码（RoPE），增强长文本建模能力，并引入Group Query Attention（GQA）机制以降低计算与显存开销，提升推理效率@bai2023qwentechnicalreport 。在预训练方面，Qwen使用覆盖中英文及多语种的大规模多领域语料，提升模型在中文和跨领域任务中的表现。训练过程中结合动态长度采样与扩展上下文窗口技术，实现最高可达32K的上下文处理能力。此外，Qwen支持参数高效微调方法（如LoRA），并全面开源，兼容主流推理与训练框架，便于下游应用部署与社区研究。

== 多模态大语言模型网络结构
本小节将简要介绍近年来主流的多模态大语言模型的网络结构设计与模型训练方法，为后续章节设计多模态大型语言模型模态对齐评估算法提供必要的背景知识。总体上多模态大型语言模型的模态融合有三种方式：分别是以BLIP-2为代表的基于Attention架构的Q-Former方法，以LLaVA为代表的直接投影法，以及以Flamingo @flamingo 为代表的将视觉信息注入到文本的生成过程中的特征极板块融合法。


=== BLIP-2
BLIP-2是一种多模态大型语言模型，旨在将预训练视觉模型的图像理解与预训练LLM的语言生成能力结合起来，以提升多模态任务性能，尤其是在开放式图文问答和图像描述等场景中。与传统端到端训练方式不同，BLIP-2采用模块化设计，通过逐阶段连接冻结的视觉编码器和语言模型，降低了预训练成本并提升了跨模态迁移能力。BLIP-2模型由三大核心组件组成：一个预训练的图像编码器、一个轻量级的跨模态投影模块（Q-former），以及一个大型语言模型。其架构如@img:qformermllm 所示 。Q-former通过查询机制提取图像的语义表示，并将其映射至语言模型可接受的输入空间，从而实现图文对齐。模型的训练包括两个关键阶段：首先是视觉语言预训练阶段），该阶段通过对视觉编码器提取的图像特征与文本进行对齐，训练一个轻量级的Q-Former来从图像中生成紧凑而富语义的表示；接着进入语言模型对齐阶段，在此阶段中，Q-Former 的输出作为提示输入，连接到一个大型预训练语言模型，通过对图文对进行进一步微调，实现多模态任务中的自然语言理解与生成能力的迁移。整个训练流程设计旨在最大程度减少视觉特征与语言模型之间的模态鸿沟，实现高效而泛化的多模态表示学习。

整个系统以冻结视觉编码器和语言模型为前提，仅训练Q-former模块，在维持性能的同时显著减少参数更新量。BLIP-2在多项多模态基准任务上取得领先性能，如Zero-shot VQA、Image Captioning 和 Visual Reasoning，展现出强大的跨模态理解与生成能力。该模型为高效训练多模态大模型提供了一种实用且具有扩展性的范式。

#figure(
  image(
    "figures/qformermllm.png",
    width: 80%,
  ),
  kind: "image",
  supplement: [图],
  caption: [单层Q-Former注意力机制的多模态大语言模型], // 英文图例
)<qformermllm>

#v(1.5em)


=== LLaVA
LLaVA是一种多模态大型语言模型，结合了预训练图像编码器与大语言模型，旨在实现多模态输入理解与自然语言生成任务。LLaVA主要聚焦于开放域图像问答、图像描述、跨模态对话等场景，具备强大的图文对齐与推理能力。LLaVA采用模块化架构，由三部分组成：一个冻结的视觉编码器、一个基于MLP神经网络的视觉特征投影模块、以及一个大型语言模型（如Vicuna、LLaMA等），其架构如@img:mlpmllm 所示。其核心思想是将图像特征映射到语言模型的输入空间，使得语言模型能够以类似处理文本的方式处理图像信息。LLaVA的训练分为两个阶段：第一个阶段在大规模图像-文本对数据集上进行投影模块参数的训练，冻结视觉编码器和LLM的参数，目的是让整体LLaVA模型达成多模态对齐，能够初步理解来自不同模态之间的信息；第二个阶段是多模态指令跟随微调阶段，通过在特定下游任务上的高质量多模态指令跟随数据集上训练投影模块和LLM的参数，可以进一步提升LLaVA模型在多模态任务上的表现。

LLaVA模型在多个多模态基准测试上表现优秀，展现了强大的跨模态理解、推理、生成能力。同时其借助MLP神经网络投影视觉特征的模态融合做法是轻量级的，显著降低了训练成本。其设计理念为多模态大语言模型的研究提供了新的思路。


#figure(
  image(
    "figures/mlpmllm.png",
    width: 80%,
  ),
  kind: "image",
  supplement: [图],
  caption: [直接投影法的多模态大语言模型], // 英文图例
)<mlpmllm>

#v(1.5em)

=== Flamingo
Flamingo 的架构基于一个冻结的预训练语言模型与一个强大的视觉编码器，中间通过插入专门设计的跨模态融合模块（Cross-Modality Gated Attention Layers）实现视觉与语言信息的高效融合。这种设计允许模型在不更新原始语言模型参数的前提下，仅微调部分跨模态层即可适应多种多模态任务，从而显著降低了训练成本。其架构如@img:flamingomllm 所示。

#figure(
  image(
    "figures/flamingomllm.png",
    width: 80%,
  ),
  kind: "image",
  supplement: [图],
  caption: [特征级融合板块的多模态大语言模型], // 英文图例
)<flamingomllm>

#v(1.5em)

== 开源多模态数据集及基准测试

=== ScienceQA

ScienceQA 是一个面向多模态科学问答任务的大规模基准数据集，旨在评估模型在科学知识理解、多模态信息融合以及推理能力方面的综合表现@scienceqa。该数据集涵盖广泛的学科领域，特别强调文本、图像和图表等多种模态信息的综合利用。ScienceQA 包含约 21,208 道多项选择题，每道题附带问题描述、选项、详细的文本讲解，并可能包含与问题相关的图像或图表等视觉信息。其中约 6,000 道题目为多模态问题，即要求模型理解图文信息的结合才能作答。数据集还标注了题目的学科类别、认知层级（如记忆、理解、应用）以及知识点标签，便于进行细粒度分析。有关ScienceQA数据集中问题的例子如@img:scienceqainfo 所示。


#figure(
  image(
    "figures/scienceqa.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [ScienceQA Benchmark示意图], // 英文图例
)<scienceqainfo>

#v(1.5em)

=== VQAv2
VQAv2（Visual Question Answering v2）是用于图像问答任务的标准基准数据集，由 Goyal 等人@vqav2 于2017年提出，旨在评估模型在视觉理解与语言推理方面的综合能力。该数据集是在VQAv1的基础上扩展而来，主要改进在于通过平衡问题答案对来缓解模型对语言偏置的依赖。VQAv2包含约265,000张来自MS-COCO@mscoco2014、约1.1M个人类标注的问题，以及每个问题对应的10个参考答案。相比VQAv1，VQAv2 对每个问题设计了图像对，使得相同的问题在不同图像上会有不同答案，从而迫使模型关注图像内容而非语言模式进行推理。

该数据集支持多种问答类型，包括是非题（yes/no）、数字类问题（number）和开放式答案（other），评估指标主要依据对10个参考答案的匹配度计算准确率。VQAv2 广泛用于评估视觉语言模型的性能，是视觉问答领域中最具代表性的基准之一。该基准数据集如@img:vqav2info 所示。


#figure(
  image(
    "figures/vqav2.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [VQAv2 Benchmark], // 英文图例
)<vqav2info>

#v(1.5em)


== 本章小结
通过本章节对多模态技术的简要介绍，本章节对于多模态对齐融合网络中的模态对齐方式及模态融合网络对各模态的数据处理方式进行了简要的梳理，为后续提出评估模态对齐程度算法的设计提供了必要的背景知识及理论支撑。


= 评估模态融合程度指标

== 问题分析
已经有相当多的工作肯定了在多模态模型中，模态融合网络确保模态对齐对于模型在下游任务上的性能的重要性 @llava @li2021alignfusevisionlanguage ，模态对齐的质量直接影响了模型在下游任务上的性能。但现有的研究中缺乏一个客观的评估指标来衡量多模态模型在训练时模态对齐程度的变化。因此对于模型模态对齐的质量的评估处于一种黑盒状态，这将影响多模态模型模态融合网络的设计与在下游任务上的性能。

本文认为，从多模态模型经过模态融合网络后产生的各模态表征的角度出发，计算各模态表征之间的统计相关性可以作为评估模态对齐程度的切入点。从神经网络学习到的表征出发，如果两个表征集合存在较高的统计相关性，则表明这两个神经网络学习到了类似的表征表示，这两个神经网络可以以相似的方式处理数据。从多模态的角度来解释，如果多模态表征之间的相关性程度较高，这表明模态融合网络在训练的过程中成功地学习到了一个多模态对齐的表征空间，各模态的知识可以有效地进行传递 @moschellarelative ；反之，如果模态表征之间的相关性程度较低，则表明模态融合网络未能学习到一个足以有效对齐多模态表征的表征空间，各模态表征还是处于各自的模态空间中且各模态的知识也未能有效地传递。

然而，由于各模态的模型产生的表征往往具有高维，互相之间强非线形的特点，对于计算表征相关性的指标适应高维和捕获复杂非线形相关性的能力提出了挑战。例如在CLIP @clip 的工作中，使用了ViT-L/14\@336px @vit 作为视觉模型的骨干，其产生的视觉表征为1024维；同时对于文本模型的骨干，使用了12个768维的Transformer层@vaswani2017attention ， 这反映了由多模态模型产生的表征高维的特点。同时由于多模态表征在初始时是由不同模态的单模态模型产生的，表征可能仍处于各自模态的维度空间中，因此不同模态表征的维数也可能并不一致，需要经过模态融合网络的训练才能够对齐到同一模态空间中，这反映出了表征之间复杂非线形关系的特点。此外，随着近年来多模态模型朝着以大型语言模型为中心的趋势发展，例如广泛使用的LLaMA模型的嵌入层为4096维，进一步加剧了多模态表征的高维特点。

因此，一个合理的评估多模态表征之间的相关性必须保证其对于高维空间的适应性及复杂非线形关系的捕获能力。在本章节的剩余部分将首先简介本征维度相关系数的原理及实验中采用的一些基线指标，之后在人造数据表征、神经网络表征、单一模态（视觉）表征、多模态（图文）表征角度上进行多种衡量表征相关性指标的充分对比实验，最终得到一个最佳的指标。该衡量表征相关性的指标将作为第四章中设计评估多模态大型语言模型模态对齐程度算法的基础。

== 研究思路与问题构建
为了解决多模态模型产生的表征之间的相关性评估问题，本文首先搜集了之前的研究中提出的多种衡量表征之间的统计相关性的指标，这些指标大多是在深度神经网络的可解释领域中研究并提出的 @klabunde2023similarity。例如：典型相关性分析（CCA） @cca ，奇异值分解典型相关性分析（SVCCA）@svcca ，中心化核对齐 （CKA） @cka ，投影加权典型相关性分析（PWCCA） @pwcca ， 距离相关系数（dCor） @dcor1 @dcor2 等，此外还有本文重点探讨的本征维度相关系数（$I_d\C\o\r$） @basile2024intrinsic 。

为了充分对比这些指标在多种条件下计算数据表征相关性的能力，本文设计了多种条件下的实验来对比这些指标的性能，具体来说，总共有四种实验条件，分别是：人造数据表征、神经网络表征、单一模态（视觉）表征、多模态（图文）表征。若记此时使用的评估指标、第一个表征矩阵、第二个表征矩阵、计算得到的相关性分别为 $Theta, X in RR^(n times d_1), Y in RR^(n times d_2), sigma$，那么将得到如下的计算公式:

$ sigma = Theta(X_(n times d_1), Y_(n times d_2)) \
Theta in \s\e\t{\m\e\t\r\i\c\s} quad sigma in [0,1]
$<corrcomputation>

在四种实验条件下得到两个表征的矩阵 $X_(n times d_1)$ 和 $Y_(n times d_2)$ 后，再依据@eqt:corrcomputation 便可以应用多种指标来计算这两个表征矩阵之间的相关性。需要指出的是，由于一些指标的实现算法需要消耗大量的显存/内存，例如中心化核对齐（CKA），合理地选取表征矩阵的行大小 $n$ （即表征数量）是重要的。

为了实现衡量多模态大型语言模型模态融合程度的目的，在通过对比实验得到一个良好的评估多模态表征相关性的指标后，还需要再根据几类多模态大型语言模型模态融合网络的特点，基于之前对比出来的良好的评估多模态表征相关性的指标去设计评估多模态大型语言模型模态对齐程度的算法。这个评估算法将考虑到实际的庞大评估数据集的计算资源消耗问题与多模态大型语言模型产生的表征的抽取及融合问题。有关这部分的主要内容将在第四章中进行解决。

// == 衡量表征相关性的基线指标的原理
// 在本小节中将主要阐述上文中提到的一些衡量表征相关性的基线指标的原理。

// 这个之后再写


== 本征维度（Intrinsic Dimension）及估算算法
本小节将主要介绍本征维度（Intrinsic Dimension）的原理、应用及估算算法，为后续3.5节阐述本征维度相关性的原理提供必要的背景知识。

本征维度最早出现在统计学的数据降维领域。高维数据集通常具有复杂的结构和特征，直接处理这些数据可能会导致计算效率低下和信息丢失。数据降维的原理是将高维数据集中的数据点投影到一个低维空间中，以便更好地理解和分析数据并简化数据的表示和处理。研究者们试图通过对高维离散数据集合的分析，来找寻嵌入在高维数据空间的本征低维流型，其包含了数据的绝大部分信息，以实现数据集降维的目的。大多数的研究者都将本征低维流型的维度视为本征维度。

本征维度的正式定义是指一个数据集的实际有效维度的数量，即可以用最少的维度来表达数据集的大部分信息。如@img:2dimension_corr 所示，呈现了三类二维数据集合，这三类二维数据集合的第一维 $X$ 与第二维 $Y$ 的关系分别是：线形相关（$X=Y$）、螺线关系（$X=theta/(6pi)cos theta，Y=theta/(6pi)sin theta，theta in [0，6pi]$）、随机采样关系（正态分布，$f(x)=1/(sqrt(2pi)sigma)e^(-(x-mu)^2/(2sigma^2))，f(y)=1/(sqrt(2pi)sigma)e^(-(y-mu)^2/(2sigma^2))$） ，每类数据集合采样为5000个数据点。不难观察到，第一类数据集合到本征维度为1，因为可以用一个变量来代表另外一个变量；第二类数据集合的本征维度为1，因为螺线关系采样出来的数据集本质上可以用一个极坐标函数 $r=r(theta)$ 表示，也只需要一个变量 $theta$ 来表示另外一个变量；而第三类数据集合的本征维度为2，因为两个变量之间没有任何关系，且两个变量均服从独立的正态分布，因此采样的数据集合需要两个变量（坐标）来表示。从这个简单的例子中，可以反映本征维度的定义，即原始的数据集合中存在冗余的无效的维度，去除那些冗余的无效的维度后，就是这个数据集合的本征维度。

#figure(
  image(
    "figures/2dimension_corr.png",
    width: 95%,
  ),
  kind: "image",
  supplement: [图],
  caption: [三类二维数据集合的本征维度], // 英文图例
)<2dimension_corr>

#v(1.5em)

对于本征维度的估算，已经有大量的研究对于计算一个数据集的本征维度提出了许多具有优异性能的方法，例如主成分分析法（PCA），Facco @twoNN 等人提出的TwoNN，Elizaveta @mle 等人提出的MLE等方法。
1. 主成分分析法通过寻找数据集中的主成分来减少数据的维度，基本思想是通过线性变换将数据投影到一个新的坐标系中，使得投影后的数据在新坐标系中的方差最大化，从而保留数据的主要特征，但这种方法在处理非线形数据流型时会因为依赖于线形变化而受限。
2. TwoNN方法的核心思想是：利用每个点的最近两个邻居之间的距离比值分布来估算本征维度。若我们记点 $p$ 距离其最近二邻的欧几里得距离分别为 $r_1$ 和 $r_2$，那么借助@eqt:neighbour_rate_twonn 可以计算得到它们之间的比例。需要注意的是，TwoNN假设数据点均匀分布在一个 $d$ 维流形上，因此 $mu$ 满足帕累托分布 $\P\a(d+1)$，其中 $d$ 代表了数据集的本征维度。因此可以通过@eqt:twonn_pareto 来计算给定的 $N$ 个样本点的分布在本征维度为 $d$ 时的条件概率。进而，我们可以通过线形回归来求解本征维度 $d$。

$
  mu = r_1 / r_2
$<neighbour_rate_twonn>

$
  P(mu|d)=d^(N) product_(i=1)^(N) mu_i^(-d-1)
$<twonn_pareto>

3. MLE的核心思想是：基于假设数据在某个低维流形上近似均匀分布，使用局部点之间的距离分布构建似然函数，从而估算该流形的维度。该方法假设了数据点在一个 $d$ 维欧几里得空间中的小邻域内均匀分布，对每个点 $x$，取它的 $k$ 个最近邻点 $x_(1), x_(2), ..., x_(k)$。假设这些点构成一个 $d$ 维球体内的样本，点之间的距离 $r_(1), r_(2), ..., r_(k)$ 可看作从一个半径为 R 的球体中随机抽取的。以这些距离为数据构建出维度 $d$ 的最大似然估计，推导出的无偏估计公式如@eqt:mle1 所示。再将所有数据点的 $hat(d)(x)$ 平均起来，得到整个数据集的本征维度估计 $hat(d)$ ，如@eqt:mle2 所示。

$
  hat(d)(x) = [1/(k-1)sum_(i=1)^(k-1)log(r_k/r_i)]^(-1)
$<mle1>

$
  hat(d) = 1/N sum_(x in X)hat(d)(x)
$<mle2>

通过仅依赖来自最近邻的本地信息，TwoNN方法不需要对数据的几何属性或密度进行详细假设，并且在应用于高维数据集时（$>=10^4$）TwoNN方法显得更直接且不易出错， 此外其计算的时间及空间复杂度为 $O(N^2)$ ，其中 $N$ 为给定的样本点的数，这表明其优异的计算资源开销。在Alessio@NEURIPS2019_cfcce062，Lorenzo@basile2024intrinsic 等人的工作中也通过实验验证了TwoNN算法的优点。本文之后的工作将主要建立在高维多模态表征中，因此TwoNN是本文选取的用于计算本征维度的方法。

== 本征维度相关性
本小节将对Lorenzo等人@basile2024intrinsic 提出的内在维度相关性 $\I_d\C\o\r$ 指标及相关理论公式进行详实的阐述。$\I_d\C\o\r$ 指标从信息论中的归一化互信息（Normalized Mutual Information - NMI）角度出发，并借助本征维度来简化了互信息中所需的熵的计算。

如果有两个数据集合 $X in RR^(n times d_1)，Y in RR^(n times d_2)$，如果想量化它们之间的相关性，归一化互信息是量化同时采样的随机变量之间关系的基本指标，其计算公式如@eqt:mutualinfo 所示 @horibe1985entropy 。但是由于“维度诅咒”的影响，在高维的数据集合中，互信息直接从选定的数据样本集合 $X$ 和 $Y$ 中计算具有挑战性，因为归一化互信息@eqt:mutualinfo 的计算需要计算熵 $H(X)$ 和 $H(Y)$，而在高维空间中，数据会迅速变得稀疏，几乎填不满空间，这会使得熵的估计变得不可靠。

$
  rho.alt = I(X, Y) / ( \m\a\x(H(X), H(Y)) ) = (H(X) + H(Y) - H(X, Y)) / ( \m\a\x(H(X), H(Y)) )
$<mutualinfo>

之前的研究工作中，有研究者证明可以使用数据集合的本征维度代替数据集熵的计算，因为本征维度拥有大部分熵的特性，并且在大规模数据集中本征维度的计算更简单迅速。Lorenzo等人通过引入数据集的本征维度来代替数据集的熵的计算，进而简化归一化互信息的计算，且将数据集合的本征维度与数据集合之间的相关性联系起来。简化后的公式便称为本征维度相关性 $\I_d\C\o\r$ 指标的计算公式，如@eqt:idcor 所示。

$
  \I_d\C\o\r(X, Y) = (\I_d (X) + \I_d (Y) - \I_d (X plus.circle Y)) / (max(\I_d (X), \I_d (Y)))
$<idcor>

$X plus.circle Y$ 表示将 $X$ 数据集合与 $Y$ 数据集合按每个数据点的维数拼接。此外，值得注意的是，不管使用哪种估计数据集合的本征维度的方法，都存在可能的误差。因此可能计算出并不准确的本征维度 $I_d$ 值，进而影响到 $\I_d\C\o\r$ 指标的可信。因此，Lorenzo等人利用了置换检验的思想，引入了一个置信指数 $p$ 来辅助计算两个数据集合之间的相关程度。置信指数 $p$ 代表着两个数据集合之间不相关的假设成立的概率。其思想是针对按照维度拼接的数据集合 $X plus.circle Y$，人为进行总计 $\s\h\u\f\f\l\e\N$ 次的行打乱（即按行更改数据点的排列），统计总共有多少次的行打乱后，$I_d (X_i plus.circle Y_i)$ 小于了原始的 $I_d (X plus.circle Y)$，其中 $X_i，Y_i$ 分别为此时行打乱后的数据集合。假设统计出来的次数为 $L$，那么置信指数 $p$ 的计算公式如@eqt:idcor_pvalue 所示。

$
  p = (L + 1) / (\s\h\u\f\f\l\e\N + 1)  
$<idcor_pvalue>

置信指数$p$的值越低，表明两个数据集合之间的相关性程度越高，反之则越低。从直觉的角度来看，如果两个数据集合之间存在较高的相关性，那么进行总计 $\s\h\u\f\f\l\e\N$ 次的行打乱后，每次的打乱使得 $I_d (X_i plus.circle Y_i) < I_d (X plus.circle Y)$ 的概率都不大，这是因为进行行打乱会破坏原有数据集合的相关性，因此针对维度拼接的数据集合 $X plus.circle Y$ 估算出来的本征维度倾向于增加（破坏原有的两个数据集合的相关性后，需要更多的维度来表示这两个数据集合拼接后的集合），因此统计出来的 $L$ 并不会很大，置信指数 $p$ 也不会很大；反之，如果两个数据集合之间本身的相关程度不高，那么进行行打乱后，可能打乱后的数据集合 $X_i，Y_i$ 在某些维度上存在一些相关性，因此针对维度拼接的数据集合 $X plus.circle Y$ 估算出来的本征维度倾向于减少（在某些维度上存在的相关性减少了拼接后的集合所需的本征维数），导致 $I_d (X_i plus.circle Y_i) < I_d (X plus.circle Y)$ 的概率会增大，进而导致统计出来的 $L$ 会较大，置信指数 $p$ 也会较高。

@eqt:idcor 和@eqt:idcor_pvalue 计算得到的 $\I_d\C\o\r$ 数值和置信指数 $p$ 统一称为 $\I_d\C\o\r$ 指标。实现 $\I_d\C\o\r$ 指标的算法伪代码如@algo:idcoralgo 所示。一般来说，$\I_d\C\o\r$ 指标的值范围在 $[0,1]$ 之间。本文之后将基于 $\I_d\C\o\r$ 指标来高效地计算多模态大型语言模型产生的表征之间的相关性，这个指标是评估多模态大型语言模型模态融合程度的算法的基础。

#[
  #import "@preview/lovelace:0.2.0": *
  #algox(
    label-name: "idcoralgo",
    caption: [$\I_d\C\o\r$指标计算算法伪代码],
    pseudocode(
      no-number,
      [#h(-1.25em) *input:* Two matrices $X in RR^(n times d_1)$ and $Y in RR^(n times d_2)$, $I_d$ estimator algorithm id_algo, and the number of shuffles $\s\h\u\f\f\l\e\N$],
      no-number,
      [#h(-1.25em) *output:* $\I_d\C\o\r$ and $p$ metric reflect the correlation between $X$ and $Y$],
      [$\i\d_1 <- "id_algo"(X)$;],
      [$\i\d_2 <- "id_algo"(Y)$;],
      [$\i\d_C <- "id_algo"("concat"(X,Y,dim=1))$;],
      [$\I_d\C\o\r <- (\i\d_1 + \i\d_2 - \i\d_C) / (max(\i\d_1,\i\d_2))$;],
      [$L <- 0;$],
      [*for* $i in {1,2,...,\s\h\u\f\f\l\e\N}$ *do*],
      ind,
      [$Y_s = "shuffle"(Y)$;],
      [$\i\d_s [i] <- "id_algo"("concat"(X, Y_S, dim=1))$;],
      [*if* $\i\d_s [i] <= \i\d_C $ *then*],
      ind,
      [$L <- L + 1$;],
      ded,
      [*end*],
      ded,
      [*end*],
      [$p <- (L + 1) / (\s\h\u\f\f\l\e\N + 1)$;],
      [*return* $\I_d\C\o\r, p$],
    ),
  )
]

#v(1.5em)



== $\I_d\C\o\r$指标与其他指标的性能对比
本小节将运用在3.2节中提到的对比实验方法进行实验，来比较多种衡量数据表征相关性的指标的性能。本小节设置了四种实验条件，分别是：人造数据表征、神经网络表征、单一模态（视觉）表征、多模态（图文）表征，以此来全面地衡量多种指标之间的性能差异，有助于我们比较出在各个领域中最优秀的指标。

=== 在人造的数据表征上评估
在这一小节中，将在人为生成的可控的数据表征上进行多种指标的性能对比实验。人为可控的含义即在构造数据表征的每个维度时，均可以硬性地控制每个维度采样的数据所服从的概率分布，如正态分布；以及不同数据表征的特定维度之间的数学表达式关系，例如 $X$ 表征集合的第一维与 $Y$ 表征集合的第一维的线形关系 $X_(\d\i\m_1) = Y_(\d\i\m_1)$。因此这些人为生成的数据表征之间的相关性在数学上是直观且可证明的，存在一个客观的真实值。这个真实值可以帮助判断各种指标计算出来的结果正确与否。

==== 一维数据表征集合之间的相关性
本节总计选取了三类人为生成的条件，每类条件都采样了两个一维数据表征集合 $X in RR^(n times 1)，Y in RR^(n times 1)$。这三类人为生成的条件中，两个一维表征集合 $X，Y$ 之间的关系分别是线形、螺旋、随机（无相关性），以数学表达式表明这三类条件如 @eqt:1dcorrexp 所示。线形关系的两个数据表征集合 $X_1，Y_1$ 中，$X_1$ 在 $[-1,1]$ 区间上均匀采样，而 $Y_1$ 完全线形依赖于 $X_1$；螺旋关系的两个数据表征集合 $X_2，Y_2$ 均非线形依赖于在 $[0,6pi]$ 区间上均匀采样的 $theta$；随机关系的两个数据表征集合 $X_3，Y_3$ 各自服从于 $mu=0，sigma^2 = 1$ 的独立正态分布。每类人为生成的条件中，$X，Y$ 均采样5000个数据点。这三类条件设置与3.4节中一致，这三类条件采样出来的数据点二维散点图如@img:2dimension_corr 所示。

$
  x_1 ~ "Uniform"(-1, 1) quad y_1 = X_1 \
  theta ~ "Uniform"(0, 6pi) quad x_2 = theta / (6pi) cos theta quad y_2 = theta / (6pi) sin theta \
  x_3 ~ "Normal"(0, 1) quad y_3 ~ "Normal"(0, 1)
$<1dcorrexp>

容易推理得到，线形关系的两个数据表征集合 $X_1，Y_1$ 之间的相关性为1，因为 $Y_1$ 完全依赖于 $X_1$；螺旋关系的两个数据表征集合 $X_2，Y_2$ 之间的相关性为1，因为可以推理得到@eqt:1dspiral 的结果，表征集合 $X_2，Y_2$ 之间存在一种非线形关系，同时 $x_2，y_2$ 变量均依赖于 $theta$，从数学角度上 $X_2，Y_2$ 表征集合的每个数据点都可以用相同的 $theta$ 表示，因此相关性程度为1；而随机关系的两个数据表征集合 $X_3，Y_3$ 之间的相关性为0，因为两个数据表征集合均服从独立的正态分布，因此没有任何相关性。

$
  x_2^2 + y_2^2 = (theta^2 cos^2 theta + theta^2 sin^2 theta) / (36pi^2) = theta^2 / (36pi^2)
$<1dspiral>

明晰了这三类条件下客观的相关性程度后，应用了多种指标来计算两个一维数据表征集合之间的相关性。具体来说，本文选取了以下指标进行对比实验：线形相关性系数（$"R"^2$），距离相关系数（dCor），$\I_d\C\o\r$ 指标。本文针对每类条件下的两个一维数据表征集合 $X，Y$，均进行了10次实验，每次实验都应用这三种指标进行评估，将这10次实验得到的指标结果进行平均后，数据如@tbl:1dcorrresult 所示，其中 $I_d plus.circle$ 代表了两个数据表征集合按维度拼接后得到的表征集合的本征维度。置信指数 $p$ 统计了10次采样的得到的最小值及最大值，以区间的形式展示。

#tablex(
  [线形关系], [$1.00plus.minus 0.00$], [$1.00plus.minus 0.00$], [$1.00plus.minus 0.00$], [$[0.01, 0.01]$], [$1.00plus.minus 0.03$],
  [螺旋关系], [$0.01plus.minus 0.00$], [$0.02plus.minus 0.00$], [$0.99plus.minus 0.02$], [$[0.01, 0.01]$], [$1.00plus.minus 0.02$],
  [随机关系], [$0.00plus.minus 0.00$], [$0.00plus.minus 0.00$], [$0.02plus.minus 0.03$], [$[0.15, 0.92]$], [$2.00plus.minus 0.03$],
  header: (
    [条件 #linebreak()],
    [$"R"^2$ #linebreak()],
    [dCor #linebreak()],
    [$\I_d\C\o\r$ #linebreak()],
    [置信指数$p$],
    [$I_d plus.circle$]
  ),
  columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
  colnum: 6,
  caption: [一维数据表征集合之间各指标计算得到的相关性（平均）],
  label-name: "1dcorrresult",
)

#v(1.5em)


从@tbl:1dcorrresult 中可以观察到，这三种指标在线形关系和随机关系上的人造数据表征集合上都能精确捕获相关性是否存在。但是在螺旋关系上，线形相关性系数（$"R"^2$）和距离相关系数（dCor）计算出来的相关性结果均为0，都无法正确地反映出两个数据表征集合之间的相关性程度，即使距离相关系数（dCor）是具有捕获非线形相关性的能力的；而 $\I_d\C\o\r$ 指标则可以正确地反映出两个数据表征集合之间的相关性程度，其相关性系数接近1，并且置信指数$p$也非常小，表明两个数据表征集合之间的相关性程度非常高。从这个实验结果中可以观察到 $\I_d\C\o\r$ 指标在捕获非线形相关性上的强大性能。


==== 四维数据表征集合之间的相关性
本文总计选取了三类人为生成的条件，每类条件都采样了两个四维数据表征集合 $X in RR^(n times 4)，Y in RR^(n times 4)$。这三类人为生成的条件中，两个四维表征集合 $X，Y$ 之间的关系分别是部分相关、完全相关、随机（无相关性），以数学表达式表明这三类条件如@eqt:4dcorrexp 所示，其中 $X_1，X_2，X_3$ 的四个维度采样都遵循 $w_1，x_1，y_1，z_1$，$Y_1，Y_2，Y_3$ 的四个维度采样分别依次遵循@eqt:4dcorrexp 中剩余的几种随机变量的概率分布。部分相关关系的两个数据表征集合 $X_1，Y_1$中，$X_1$ 的四个维度各自独立地服从 $mu = 0, sigma^2 = pi^2$ 的正态分布，而 $Y_1$ 的后两个维度各自独立地服从 $mu = 0, sigma^2 = pi^2$ 的正态分布，但是前两个维度非线形依赖于 $X_1$ 的部分维度；完全相关的两个数据表征集合 $X_2，Y_2$ 中，$X_2$ 的四个维度各自独立地服从 $mu = 0, sigma^2 = pi^2$ 的正态分布，$Y_2$ 的四个维度全部非线形依赖于 $X_2$ 的部分维度；随机关系的两个数据表征集合 $X_3，Y_3$ 中，$X_3$ 和 $Y_3$ 的四个维度都各自服从于 $mu=0，sigma^2 = pi^2$ 的独立正态分布。每类人为生成的条件中，$X，Y$ 均采样5000个数据点，这与3.6.1.1节中的采样数量是一致的。


$
  & w_1 ~ "Normal"(0, pi^2) quad x_1 ~ "Normal"(0, pi^2) quad y_1 ~ "Normal"(0, pi^2) \
  & z_1 ~ "Normal"(0, pi^2) \
  & w_2 ~ "Normal"(0, pi^2) quad x_2 ~ "Normal"(0, pi^2) quad y_2 ~ "Normal"(0, pi^2) \
  & z_2 ~ "Normal"(0, pi^2) \
  & w_3 = cos(w_1 + y_1) quad x_3 = sin(x_1) quad y_3 ~ "Normal"(0, pi^2) \
  & z_3 ~ "Normal"(0, pi^2) \
  & w_4 = cos(w_1 + y_1) quad x_4 = sin(x_1) quad y_4 = sin(x_1 z_1) \
  & z_4 = cos(y_1) \
$<4dcorrexp>


可以推理得到，部分相关的两个数据表征集合 $X_1， Y_1$ 之间存在部分的非线形相关性，因为 $Y_1$ 的前两个维度 $X_1$ 的部分维度之间呈现了三角函数（非线形）依赖；完全相关的两个数据表征集合 $X_1，Y_1$ 之间存在很强的非线形相关性，因为 $Y_2$ 的四个维度全部与 $X_2$ 的部分维度之间呈现了三角函数（非线形）依赖；随机关系的两个数据表征集合 $X_3，Y_3$ 之间不存在任何相关性，因为 $X_3，Y_3$ 的所有维度都是独立采样的。


确定了这三类条件下的客观的相关性程度后，应用了多种指标来计算两个四维数据表征集合之间的相关性。具体来说，本文选取了典型相关性分析（CCA），距离相关系数（dCor）， $\I_d\C\o\r$ 指标。本文针对每类条件下的两个四维数据表征集合 $X，Y$ 均进行了5000次的实验，每次实验都应用这三种指标进行评估，将这5000次实验得到的指标结果进行平均后，数据如@tbl:4dcorrresult 所示，其中 $I_d plus.circle$ 代表了两个数据表征集合按维度拼接后得到的表征集合的本征维度。置信指数 $p$ 统计了5000次实验中得到的最小值及最大值，以区间的形式展示。


#tablex(
  [部分相关], [$0.05plus.minus 0.01$], [$0.00plus.minus 0.00$], [$0.35plus.minus 0.04$], [$[0.01, 0.01]$], [$6.41plus.minus 0.12$],
  [全部相关], [$0.05plus.minus 0.01$], [$0.01plus.minus 0.00$], [$0.67plus.minus 0.03$], [$[0.01, 0.01]$], [$4.91plus.minus 0.10$],
  [随机关系], [$0.04plus.minus 0.01$], [$0.00plus.minus 0.00$], [$0.02plus.circle 0.04$], [$[0.01, 1.00]$], [$7.98plus.circle 0.14$],
  header: (
    [条件 #linebreak()],
    [CCA #linebreak()],
    [dCor #linebreak()],
    [$\I_d\C\o\r$ #linebreak()],
    [置信指数$p$],
    [$I_d plus.circle$]
  ),
  columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
  colnum: 6,
  caption: [四维数据表征集合之间各指标计算得到的相关性（平均）],
  label-name: "4dcorrresult",
)

#v(1.5em)

从@tbl:4dcorrresult 中可以观察到，在随机关系的条件下，这三种指标均可以正确地反映出两个数据表征集合之间的相关性程度，且计算出来的相关性系数均接近0，每次采样得到的 $\I_d\C\o\r$ 指标的置信指数 $p$ 也表现出了很大的波动性，且大部分呈现为较高的值，成功地反映出两个数据表征集合之间的相关性程度非常低；而在部分相关和完全相关的条件下，线形相关性系数（CCA）和距离相关系数（dCor）计算出来的相关性结果都非常低，均无法正确地反映出两个数据表征集合之间的非线形相关性关系，但是 $\I_d\C\o\r$ 指标则可以正确地反映出两个数据表征集合之间的相关性程度，在5000次实验的结果中，在部分相关的条件下， $\I_d\C\o\r$ 指标的相关性系数的均值为0.35，在完全相关的条件下，$\I_d\C\o\r$ 指标的相关性系数的均值为0.67，并且这两种条件下的置信指数 $p$ 也非常小，表明两个数据表征集合之间存在明确的相关性。

尽管距离相关指数（dCor）是一种非线形相关性评估指标，但是在较为复杂的非线形关系的条件下表现不佳（即本实验中的部分相关条件和全部相关条件）。但是 $\I_d\C\o\r$ 指标在捕获复杂非线形相关性时表现良好，成功地捕获了部分相关和完全相关的条件下两个数据表征集合之间的复杂非线形关系。


=== 在神经网络产生的表征上评估
深度学习技术的核心是深度神经网络。通过神经网络的多层非线形变换，可以将原始的输入数据映射为一个复杂的高维空间中的表征。神经网络的前向传播计算过程如@eqt:mlpforward 所示，其中 $X_0 in RR^(n times d)$ 代表了输入数据矩阵，$W_i$ 代表了神经网络层中每层的线形变换，$b_i$ 代表了每层神经元的偏置矩阵，$sigma$ 代表了每个隐藏层的激活函数。


$ 
  H = X_(L-1) W_(L) + b_(L) \
  X_i = sigma(X_(i-1) W_i + b_i), quad X_0 in RR^(n times d) \
  quad W_i in RR^(d_1, d_2), quad b_i in RR^(1 times d_2) \
$<mlpforward>


事实上，通过每个网络层的线形变换及非线形激活函数（例如ReLU，GeLU，Sigmoid等），神经网络可以学习到原始数据输入与输出之间复杂的非线形函数，因此神经网络输出的表征实际上是对原始数据输入应用了学习到的非线形函数的结果，因此原始输入数据表征与神经网络输出的表征之间以一种复杂的非线形关系相互关联。鉴于这个客观存在的非线形相关性，衡量表征相关性的指标应该具有捕获这种相关性的能力。

在这一小节中，在原始输入数据的表征和经过MLP神经网络产生的表征上，应用了多种衡量表征相关性的指标去计算这两个表征之间的相关性，以此来对比不同指标之间的性能。

==== 实验设置
本实验的原始输入数据选取了MNIST @mnist 手写数字体图片数据集，该数据集是一个图像分类数据集，共有10类，对应于10个数字，每类的图像是由 $28 times 28$ 大小的手写数字体图像构成的，该数据集如@img:mnistshow 所示；神经网络选取了具有15层全连接的隐藏层，每个隐藏层具有784（对应手写数字体图片 $28 times 28$）个神经元，输出层具有10个神经元（对应 $0-9$ 的数字）的MLP神经网络。

#figure(
  image(
    "figures/mnistshow.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [MNIST手写数字体识别数据集示意图], // 英文图例
)<mnistshow>

#v(1.5em)

在MLP神经网络每层隐藏层中，每层采用了同样的激活函数，即泄漏整流线性单元（Leaky ReLU）@maas2013rectifier。Leaky ReLU是一种常用的非线性激活函数，旨在缓解传统 ReLU 函数存在的“神经元死亡”问题。标准ReLU在输入小于零时输出恒为零，导致部分神经元在训练过程中无法更新，从而“失活”。相比之下，Leaky ReLU 在负输入区域引入了一个很小的非零斜率 $alpha$ （通常设为 0.01），其计算公式如@eqt:leakyrelu 所示。

$
  "Leaky ReLU"(x) = cases(
    x quad x >= 0,
    alpha x quad x < 0,
  )
$<leakyrelu>

Leaky ReLU激活函数是一个在 $x = 0$ 处分段的分段函数。在 $x < 0$ 时，函数值由直线 $y = alpha x$ 表示，其中斜率 $alpha$ 参数是可以人为设置的参数；在 $x >= 0$ 时，函数值由直线 $y = x$ 表示。Leaky ReLU激活函数随着斜率 $alpha$ 趋近于0时的函数图像变化如@img:leakyrelu 所示。

#figure(
  image(
    "figures/leakyrelu.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [Leaky ReLU激活函数随着 $alpha$ 参数变化的图像变化], // 英文图例
)<leakyrelu>

#v(1.5em)

不难观察到，若斜率 $alpha$ 的值从1逼近于0的过程中，Leaky ReLU激活函数的非线形程度将会增加。当 $alpha=1$ 时，此时作用Leaky ReLU激活函数的神经网络将只能对输入数据进行线形变换，这是因为此时的Leaky ReLU激活函数为线形函数 $y=x$，而神经网络的全连接层也只能对于上一层计算得到的表征做线形变换 $X_(i-1)W_i$，因此整体上作用 $alpha=1$ 的Leaky ReLU激活函数的神经网络就是对初始的数据做了一次线形变换；当 $alpha$ 的值逐渐减小时，由于Leaky ReLU激活函数的非线形程度增加，神经网络最终输出的表征是对初始数据作用了非线形程度更高的变换的结果。

本实验利用了Leaky ReLU激活函数的特性，通过控制 $alpha$ 的值来控制神经网络对数据表征作用的变换的非线形程度。具体来说，本实验对 $[1.00-0.00]$ 这个区间以步长-0.02均匀地选取 $alpha$ 的值，并让神经网络对输入数据MNIST手写数字体数据执行前向传播。并抽取输入数据的原始表征和神经网络最后一个隐藏层输出的表征，对这两个表征集合进行相关性评估。在本实验中，选取了距离相关性系数（dCor），中心化核对齐 （CKA），奇异值分解典型相关性分析（SVCCA），$\I_d\C\o\r$ 指标四种指标进行对比实验。



==== 实验结果与分析

应用在3.6.2.1小节中选取的几种指标得到的实验结果如@img:mnistexp 所示。可以发现，在斜率 $alpha$ 的值接近于1时，由于Leaky ReLU激活函数的非线形程度不高，大部分的指标都能准确的计算出来，除了SVCCA指标，因为该指标主要依赖线形变换的方法来计算表征之间的相关性，难以捕获到非线形相关性。但是随着斜率 $alpha$ 的值逐渐趋近于0，Leaky ReLU激活函数的非线形程度逐渐增加，其函数形状也趋近于类似ReLU的阶跃函数时，其他的指标会因为该趋势而对相关性的计算造成影响。可以观察到，dCor指标和CKA(linear&&rbf)指标对于相关性的衡量随着 $alpha$ 的变化有一个显著的下降趋势，这表明这些指标对于复杂非线形相关关系的捕获能力较弱，这也与在3.6.1.2节中探讨的一致，即针对复杂非线形关系的鲁棒性不佳；此外也可以观察到这些指标计算出来的相关性系数大致在 $[0.5, 0.8]$ 的区间内，并没有很好地衡量出这两个表征之间的相关性。

但是 $\I_d\C\o\r$ 指标随着斜率 $alpha$ 的值接近于0时，下降的趋势相对而言较小，且在整个斜率 $alpha$ 的变化过程中，$\I_d\C\o\r$ 指标计算出来的相关性系数大致在 $[0.8, 1.0]$ 的区间内。这表明 $\I_d\C\o\r$ 指标不仅能够准确地捕获到表征间复杂的非线形关系，且对复杂的非线形关系的鲁棒性也较强；同时也能准确地衡量出这两个表征之间存在的相关性。

在本小节的实验中，再次通过对比实验印证了 $\I_d\C\o\r$ 指标在捕获复杂非线形相关性方面的强大性能，且在计算相关系数时对于非线形程度的增强具有鲁棒性。

#figure(
  image(
    "figures/mnist.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [对mnist经过MLP神经网络产生的表征计算相关性指标], // 英文图例
)<mnistexp>

#v(1.5em)

=== 在视觉模态表征上进行评估
本小节的实验从更实际、真实的角度出发，衡量不同视觉模型在图像分类数据集ImageNet@imagenet 上前向传播得到的表征之间的相关性。在同一个数据集上，应用不同的视觉模型对其进行推理时产生的表征实际上是对原始的数据表征做了不同的非线形变换得到的结果，因此视觉模型产生的表征之间是存在客观的相关性的。一个合理的衡量表征之间相关性的指标应该能够有效地捕获这种相关性；同时实际的深度学习神经网络模型所产生的表征都是高维的，这也给衡量表征之间相关性的评估指标带来了挑战，能够克服高维带来的负面影响是衡量指标性能的一个关键因素。在本小节中进行对应的对比实验。


==== 实验设置
具体而言，本实验的原始数据集选取的是ImageNet数据集。ImageNet是一个大规模图像识别及分类数据集，广泛用于计算机视觉和深度学习研究。本实验使用的是其子集ImageNet-1k，包含1000个图像分类类别和超过120万张训练图像。ImageNet数据集如@img:imagenetshow 所示。


#figure(
  image(
    "figures/imagenet.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [ImageNet数据集示意图], // 英文图例
)<imagenetshow>

#v(1.5em)

本实验的视觉模型选取的是开源预训练模型。具体而言，本实验选取了两种基于卷积的神经网络：ResNet-18@resnet 和EfficientNet-B0@efficientnet ；四种基于视觉Transformer的神经网络，即ViT模型系列@vit ：ViT-B-16-224，ViT-B-32-384，ViT-L-16-224，ViT-CNN hybrid；以及两种基于自监督学习方法的视觉语言模型（视觉模型基于ViT）：CLIP-ViT-B@clip 和SigLIP-ViT-B@siglip 。对于上面所述的几种基于卷积和视觉Transformer的视觉模型，本实验抽取了最后一个隐藏层输出的表征（即卷积神经网络的池化层，ViT模型中的最后一个分类token）；针对自监督学习的CLIP和SigLIP模型，本实验抽取了视觉与文本共享语义空间中的视觉表征。在本实验中，针对在不同模型上抽取的视觉表征，应用了大小为30000的相同随机排列来缩减表征集合的行数，这是因为过多的表征会导致应用指标计算相关系数时的计算代价过高，例如中心化核对齐（CKA）等指标。表征集合的行数为30000是一个合理的数值，兼顾了充足的统计量和适当的计算代价。

本实验选取的指标为：距离相关性系数（dCor），中心化核对齐 （CKA），奇异值分解典型相关性分析（SVCCA），$\I_d\C\o\r$ 指标四种指标进行对比实验。

==== 实验结果与分析

实验结果如@img:imagenetbaselineexp 、@img:imagenetidcorexp @tbl:imagenetmean 所示。@tbl:imagenetmean 中的结果是不同指标在@img:imagenetbaselineexp 和@img:imagenetidcorexp 中计算得到的矩阵中去除对角线上的元素而计算得到的平均值（因为对角线上的元素是同种视觉模型产生的表征之间衡量相似度，并没有计算的实际意义）。可以观察到，所有的指标均可以捕获到不同视觉模型产生的表征之间的一些相关性，但是一些基线指标反映出来的值并不高。例如dCor指标在计算视觉模型EfficientNet-B0和CLIP（其中的视觉模型）产生的表征之间的相关性时，其计算出来的值为0.29，这个较低的值并不能很好的反映出这两个表征集合之间的相关性，因为这两个表征本质上都是对原始数据表征做了非线形变换从而得到的结果。从@tbl:imagenetmean 展示的不同指标计算得到的非对角线元素的平均值中可以观察到，基线指标中表现最好的是dCor指标，其平均值在基线指标中达到了最高，为0.51；而 $I_d\C\o\r$ 指标的平均值达到了0.88，远高于其他所有指标的平均值，比较@img:imagenetidcorexp 和@img:imagenetbaselineexp 中的结果也可以观察得到类似的结论。同时置信指数 $p$ 也较低，表明两个视觉表征集合之间没有相关性的可能性不高。这表明 $I_d\C\o\r$ 指标在捕获视觉模型产生的表征之间的相关性方面表现优异，尤其是在处理复杂的非线性关系时。

#figure(
  grid(
    columns: 2,
    image("figures/dcor.svg", width: 100%),
    image("figures/svcca.svg", width:100%),
    image("figures/linear_cka.svg", width: 100%),
    image("figures/rbf_cka.svg", width: 100%),
  ),
  kind: "image",
  supplement: [图],
  caption: [基线指标对视觉模型产生的表征之间的相关性程度计算（ImageNet）], // 英文图例
)<imagenetbaselineexp>

#v(1.5em)

#figure(
  grid(
    columns: 2,
    image("figures/idcor.svg",width: 100%),
    image("figures/pvalues.svg",width: 100%),

  ),
  kind: "image",
  supplement: [图],
  caption: [$I_d\C\o\r$ 指标对视觉模型产生的表征之间的相关性程度计算（ImageNet）], // 英文图例
)<imagenetidcorexp>

#v(1.5em)

#tablex(
  [dCor], [0.51],
  [SVCCA], [0.45],
  [linear CKA], [0.43],
  [rbf CKA], [0.44],
  [$\I_d\C\o\r$], [*0.88*],
  header: (
    [评估指标 #linebreak()],
    [平均值 #linebreak()],
  ),
  columns: (1fr, 1fr),
  colnum: 2,
  caption: [ImageNet上不同指标计算视觉模型产生的表征之间的相似性（非对角线平均值）],
  label-name: "imagenetmean",
)

#v(1.5em)

==== 粗对齐数据情况下相关性的评估
在本小节（3.6.3）之前的实验中，计算不同的视觉模型在同一个原始输入数据上产生的表征之间的相关性时，是一种表征之间绝对对齐的情况。这里对齐的含义指的是两个数据表征集合的每一行虽然由不同的视觉模型产生，导致视觉表征不同，但是它们都源于同一张原始输入数据（即同一张图片），因此最终的两个表征集合之间存在一种客观的相关性，这种情况称为精细对齐。本小节从另外的角度：粗略对齐出发，来探讨不同指标在数据是粗略对齐的情况下性能将如何变化。本实验引入粗略对齐的条件的目的是为了探讨各种指标在不太理想的情况下的性能表现，这是有实际意义的，因为实际的原始数据集中可能存在一些噪声数据，如果衡量相关性的指标能够在有噪声的情况下表现良好将有助于进一步的应用。

具体而言，本实验利用了ImageNet图像分类数据集中图片所属类别的这个固有特性，将每个视觉模型产生的表征集合 $X$ 进行类内混洗，以此来达成一种粗略对齐的效果。直观上，这么做的效果是将同一类的不同图片的视觉表征配对在一起了，此时的粗对齐视觉表征对之间的相关性由同一个类别所保证。具体来说，本实验将首先统计视觉模型产生的表征集合 $X$ 中的所有类别，并记录下每个类别的表征在集合中的行索引，之后再混洗每个类别的行索引集合以此来打乱表征集合的行排列。这样的结果便是表征集合 $X$ 的每一行都与同一类的其他行所配对，即给定行索引 $i$，假如此行所属的类别为 $C_i$，那么所配对的行 $j$ 需要满足 $C_i = C_j$。通过这种混洗方式得到新的表征集合 $Y$，并将表征集合 $X$ 与 $Y$ 应用多种指标的评估。由于这种混洗方式破坏了原有的精细对齐关系，可以预见地将导致相关性指标评估的结果下降。此外，本实验还探讨了完全随机混洗的条件下应用多种指标的结果，即未强制约束配对时的行的所属类别。所有的实验结果如@tbl:imagenetcoarse 所示，其中类内混洗用(s)表示，完全随机混洗用(r)表示。

可以观察到，在类内混洗的条件下，$I_d\C\o\r$ 指标仍旧能够计算出较高的相关性值，在多个视觉模型产生的表征集合上均计算出了最高的相关性系数值；且两个数据表征集合之间没有相关性的置信指数 $p$ 也较低。这表明 $I_d\C\o\r$ 指标在存在一定噪声和粗对齐的条件下仍旧能够表现良好。此外，在完全随机混洗的条件下，$I_d\C\o\r$ 指标计算出来的相关性系数值偏低，且置信指数 $p$ 也较高，较好地反映出了两个数据表征集合之间的弱相关性。


#tablex(
  [$\I_d\C\o\r$(s)], [*0.59*], [0.54], [*0.68*], [*0.7*], [*0.62*], [*0.73*], [*0.66*], [0.54], 

  [$p_(\v\a\l)$(s)], [0.01], [0.01], [0.01], [0.01], [0.01], [0.01], [0.01], [0.01],

  [dCor(s)], [0.26], [*0.59*], [0.45], [0.48], [0.56], [0.56], [0.38], [*0.62*], 

  [linear_cka(s)], [0.17], [0.58], [0.34], [0.40], [0.52], [0.46], [0.34], [0.59], 

  [rbf_cka(s)], [0.19], [*0.59*], [0.35], [0.40], [0.53], [0.44], 
  [0.35], [0.61], 

  [SVCCA(s)], [0.28], [0.28], [0.40], [0.40], [0.40], [0.41], 
  [0.23], [0.27],

  [$\I_d\C\o\r$(r)], [-0.11], [0.00], [0.27], [0.23], [0.27], [0.37], [0.41], [0.21],

  [$p_(\v\a\l)$(r)], [0.24], [0.33], [0.35], [0.65], [0.59], [0.48], [0.47], [0.41],

  header: (
    [评估指标 #linebreak()],
    [EfficientNet #linebreak()],
    [SigLIP #linebreak()],
    [ViT-B-16 #linebreak()],
    [ViT-B-32 #linebreak()],
    [ViT-hyb #linebreak()],
    [ViT-L #linebreak()],
    [Resnet #linebreak()],
    [CLIP #linebreak()],
  ),
  columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
  colnum: 9,
  caption: [类内混洗（s）与完全随机混洗（r）下的指标结果],
  label-name: "imagenetcoarse",
)

#v(1.5em)


从实际的深度学习视觉模型的角度，本小节（3.6.3）再次通过对比实验验证了 $I_d\C\o\r$ 指标相比其他基线指标的在捕获非线形关系上的强大性能以及面对高维表征及噪声数据时的鲁棒性。


=== 在多模态表征上进行评估
在3.6.3小节中，本文已经证明了 $I_d\C\o\r$ 指标在评估视觉模态表征时的优秀表现。然而这种表征是单一模态的，为了充分证明 $I_d\C\o\r$ 指标可以有效地应用于评估多模态大型语言模型模态融合程度的算法，需要确保 $I_d\C\o\r$ 指标在多模态领域中也能有效地衡量不同模态模型在不同维度的潜在空间中产生的表征之间的相关性，本小节将对比不同指标在多模态表征上进行评估的性能，这里的多模态指的是图像文本之间的多模态。

==== 实验设置
本实验选取了两个多模态数据集：MS-COCO 2014@mscoco2014 中的Caption数据集和N24News@n24news。这两个数据集均为图像-标题（文本）对形式的多模态数据集。

MS-COCO Caption 数据集专注于图像描述生成任务。该数据集中每张图像均配有由人类标注的自然语言描述，涵盖图像中物体的类别、属性、关系以及上下文信息，内容丰富且语言自然。其多样性和高质量的注释使其在多模态学习、视觉语言预训练等研究方向中具有重要地位。N24News是一个专为多模态新闻分类任务设计的数据集，源自《纽约时报》的新闻报道。该数据集每条新闻配有一张图像及其对应的标题、摘要、正文和图像说明（caption）。在图像描述（caption）任务方面，N24News 的图像说明由专业编辑撰写，准确反映图像中的关键要素和上下文信息。

#figure(
  grid(
    columns: 2,
    image("figures/cococapshow.jpg",width: 100%),
    image("figures/n24newsshow.png",width: 100%),

  ),
  kind: "image",
  supplement: [图],
  caption: [MS-COCO 2014 Caption[左]与N24News[右]数据集示意图], // 英文图例
)<cococapshow>



本实验选取的预训练视觉模型为：两种基于卷积的神经网络：ResNet-18@resnet 和EfficientNet-B0@efficientnet ；两种基于视觉Transformer的神经网络@vit ：ViT-B-16-224，ViT-CNN hybrid；以及一种基于自监督学习方法的多模态视觉语言模型：CLIP-ViT-B@clip 中的视觉模型分支。本实验选取的预训练文本模型为：两种BERT系列的文本模型@bert Bert-Cased、Bert-UnCased；ALBERT@albert ；Electra@electra ；以及多模态视觉语言模型：CLIP-ViT-B@clip 中的文本模型分支。

本实验计算视觉和文本模态的模型在多模态数据集上针对各自模态的数据产生的表征之间的相关性。由于多模态图文对数据集中的图像标题经过人工的标注已经能够完美地用自然语言描述图像中的内容及内容之间的逻辑关联，因此可以预料的，经过视觉模型和文本模型产生的表征之间是存在客观的相关性的，因为图像标题的自然语言描述与图像内容达成了语义上的一致（对齐）。

从具体抽取视觉及文本模态的模型产生的表征的角度来说，视觉模态的模型的抽取表征方式与3.6.3小节中的方式相同；本实验针对Bert、ALBERT、Electra等文本模态的模型，抽取了模型在最后一个隐藏层的输出表征的分类token（cls token），这是因为该token是上述模型对整段文本的整体理解的表征，上述模型在训练的过程中对于该token进行专门的优化，使得该token能够较好地表征文本的全局语义信息；针对CLIP-ViT-B的文本模型分支，抽取了视觉与文本共享语义空间中的文本表征。

在本实验中，选取了距离相关性系数（dCor），中心化核对齐 （CKA），奇异值分解典型相关性分析（SVCCA），$\I_d\C\o\r$ 指标四种指标进行对比实验。


==== 实验结果与分析
在MSCOCO多模态数据集上的实验结果如@img:cocobaselineexp 、@img:cocoidcorexp 、@tbl:cocomean 所示。在N24News多模态数据集上的实验结果如@img:n24newsbaselineexp 、@img:n24newsidcorexp 、@tbl:n24newsmean 所示。从@tbl:cocomean 中展示的不同指标计算得到的非对角线元素的平均值中可以观察到，衡量多模态表征的相关性程度时，表现最好的基线指标为dCor，其平均值达到了0.33，而 $I_d\C\o\r$ 指标的平均值达到了0.64，远高于其他的全部基线指标。在@tbl:n24newsmean 中也可以观察到类似的结论。这表明 $I_d\C\o\r$ 指标在捕获多模态表征之间的相关性方面的强大能力。同时在@img:cocobaselineexp 和 @img:cocoidcorexp 中可以观察到基线方法计算出来的多模态表征相关性数值矩阵的热力图呈现了明显的“块状”结构，即基线指标只能捕获到同种模态表征之间的相关性，对于不同种模态之间的相关性则几乎无法捕获。而 $I_d\C\o\r$ 指标则能够有效地捕获到不同模态表征之间的相关性。因此本实验得出了这么一个结论：在多模态表征之间的相关性评估中，本实验所选取的几种相关性指标中只有 $I_d\C\o\r$ 指标能够有效地捕获到不同模态表征之间的相关性。该结论同样可以在@img:n24newsbaselineexp 和@img:n24newsidcorexp 中观察得到。此外，$I_d\C\o\r$ 指标计算出来的置信指数$p$也较低，表明两个模态表征集合之间没有相关性的假设成立的概率不高。


#figure(
  grid(
    columns: 2,
    image("figures/idcormm/coco/dcor.svg", width: 100%),
    image("figures/idcormm/coco/svcca.svg", width:100%),
    image("figures/idcormm/coco/linear_cka.svg", width: 100%),
    image("figures/idcormm/coco/rbf_cka.svg", width: 100%),
  ),
  kind: "image",
  supplement: [图],
  caption: [基线指标对多模态表征之间的相关性程度计算（MSCOCO）], // 英文图例
)<cocobaselineexp>

#v(1.5em)

#figure(
  grid(
    columns: 2,
    image("figures/idcormm/coco/idcor.svg",width: 100%),
    image("figures/idcormm/coco/pvalues.svg",width: 100%),

  ),
  kind: "image",
  supplement: [图],
  caption: [$I_d\C\o\r$ 指标对多模态表征之间的相关性程度计算（MSCOCO）], // 英文图例
)<cocoidcorexp>

#v(1.5em)

#tablex(
  [dCor], [0.33],
  [SVCCA], [0.29],
  [linear CKA], [0.29],
  [rbf CKA], [0.31],
  [$\I_d\C\o\r$], [*0.64*],
  header: (
    [评估指标 #linebreak()],
    [平均值 #linebreak()],
  ),
  columns: (1fr, 1fr),
  colnum: 2,
  caption: [MSCOCO上不同指标计算多模态表征之间的相似性（非对角线平均值）],
  label-name: "cocomean",
)

#v(1.5em)

#figure(
  grid(
    columns: 2,
    image("figures/idcormm/n24news/dcor.svg", width: 100%),
    image("figures/idcormm/n24news/svcca.svg", width:100%),
    image("figures/idcormm/n24news/linear_cka.svg", width: 100%),
    image("figures/idcormm/n24news/rbf_cka.svg", width: 100%),
  ),
  kind: "image",
  supplement: [图],
  caption: [基线指标对多模态表征之间的相关性程度计算（N24News）], // 英文图例
)<n24newsbaselineexp>

#v(1.5em)

#figure(
  grid(
    columns: 2,
    image("figures/idcormm/n24news/idcor.svg",width: 100%),
    image("figures/idcormm/n24news/pvalues.svg",width: 100%),

  ),
  kind: "image",
  supplement: [图],
  caption: [$I_d\C\o\r$ 指标对多模态表征之间的相关性程度计算（N24News）], // 英文图例
)<n24newsidcorexp>

#v(1.5em)

#tablex(
  [dCor], [0.29],
  [SVCCA], [0.27],
  [linear CKA], [0.25],
  [rbf CKA], [0.27],
  [$\I_d\C\o\r$], [*0.66*],
  header: (
    [评估指标 #linebreak()],
    [平均值 #linebreak()],
  ),
  columns: (1fr, 1fr),
  colnum: 2,
  caption: [N24News上不同指标计算多模态表征之间的相似性（非对角线平均值）],
  label-name: "n24newsmean",
)

#v(4.0em)

== 本章小结
本章首先从设计多模态大型语言模型模态融合程度评估算法时所遇到的实际问题出发：表征来自不同模态模型的encoder；表征往往是高维的；表征之间存在难以捕获的复杂非线形关系，引出了需要一个能够在高维空间中准确捕获表征间非线形关系，且能够在多模态领域中对语义对齐的多模态表征准确捕获相关性的指标的需求。接着本章引入了几种基线指标：典型相关性分析（CCA） @cca ，奇异值分解典型相关性分析（SVCCA）@svcca ，中心化核对齐 （CKA） @cka ，投影加权典型相关性分析（PWCCA） @pwcca ， 距离相关系数（dCor），以及重点介绍了数据表征集合的本征维度概念以及 $I_d\C\o\r$ 指标的原理和算法实现。

本章重点设计了衡量 $I_d\C\o\r$ 指标与其他基线指标之间的性能对比实验，通过在人造数据表征、神经网络表征、单一模态（视觉）表征、多模态（图文）表征的角度上进行全面充分的对比表征相关性指标之间的性能差异实验，并进行了详实的数据分析。通过对比实验，最终实验结果表明： $I_d\C\o\r$ 指标在捕获表征之间的相关性方面表现优异，尤其是在处理复杂的非线性关系时，并且在高维表征及多模态表征上都表现优异，同时对于存在一定噪声的数据时也具有良好的鲁棒性。这些良好的性能特征都是其他指标所不具有的。本章成功验证了 $I_d\C\o\r$ 指标在不同模态表征上的可靠性，为多模态大模型的模态对齐评估提供了有效的工具。




= 评估多模态大型语言模型模态对齐的算法

== 问题背景及研究思路
在第3.1节与第3.2节中已经阐述了评估多模态大型语言模型模态对齐程度的重要性以及设计相关评估算法时将会面临的实际问题，如多模态表征处于各自模态的潜空间中，这些潜空间的维度往往是高维的，且不同模态的潜空间维度可能不相等；表征之间存在复杂的非线形关系；表征的多模态属性等。但是第三章的工作已经通过对比实验为本文筛选出来一个足以适应高维的多模态表征，具有强大的复杂非线形关系捕获能力，且对数据中存在的噪声鲁棒的评估指标 $I_d\C\o\r$。因此本章将 $I_d\C\o\r$ 指标作为设计一个评估多模态大型语言模型模态对齐程度的算法的核心。

在近期多模态大型语言模型的研究中，以视觉-语言多模态大型语言模型为例，研究者们将研究的重点放在了模态融合网络的设计上。在训练时冻结全部或大部分的视觉模型参数及文本模型参数，主要训练模态融合网络的参数。通过这种方式，模态融合网络可以将视觉模型产生的视觉表征对齐到文本模型的语义空间中，从而帮助文本模型理解视觉模态的信息，以此完成多模态任务。模态融合网络对齐一个模态的表征到另一个模态的语义空间的流程示意图可以参见@img:mllm 。

事实上，在训练模态融合网络的过程中，模态融合网络将一个模态（例如视觉模态）的表征对齐到另一个模态（例如文本模态）的语义空间中的能力是越来越强的，这可以体现在模态融合网络对一个模态的表征处理后所输出的表征与目标对齐模态的表征之间的相关性系数。客观来说，如果模态融合网络对齐的程度足够好，那么经过模态融合网络处理后的表征与目标模态的表征之间能够形成一种语义上的对齐关系，因此处理后的表征与目标模态的表征之间的相关性系数也会比较高；反之，如果模态融合网络对齐的程度不够好，那么就无法达成一种语义上的对齐关系，处理后的表征与目标模态的表征之间的相关性系数会比较低。

基于这个事实，本章所设计的评估多模态大型语言模型的模态对齐程度的算法主要利用了经过模态融合网络后产生的两个模态的表征集合，并将这两个模态的表征集合作为输入送入本章设计的evalmm算法。

== evalmm模态对齐评估算法
基于在4.1节中阐述的研究思路，本节将设计一个评估多模态大型语言模型模态对齐程度（以图像文本模态为例）的算法*evalmm*（*Eval*\uate *M*\ultiModal Large Language Model *M*\odality Alignment）。evalmm算法的执行步骤如下四个步骤所示，计算过程示意图如@img:evalmm 所示，evalmm算法的伪代码实现如@algo:evalalgo 所示。本小节将对这四个步骤进行详细的阐述。

1. 准备步骤。evalmm算法需要首先确定一个评估模态对齐程度的数据集，这个数据集需要是图像-标题对形式的多模态数据集，例如MS-COCO，N24News等人工标注的开源多模态数据集，因为这些数据集中的图像标题对已经达成了准确的语义对齐。


2. 模态特征抽取与融合。基于评估模态对齐程度的数据集，抽取经过模态融合网络处理后的视觉特征和文本特征，此时的视觉特征和文本特征可能具有多个嵌入向量embedding（以LLaVA模型为例，经过模态融合网络MLP投影后的视觉特征具有576个嵌入向量，文本特征虽然不经过模态融合网络MLP，其嵌入向量数量与其序列化后的token数量一致），因此需要融合所有的视觉特征为一个视觉表征，融合所有的文本特征为一个文本表征，以此来方便后续使用 $I_d\C\o\r$ 指标进行表征相关性的衡量。evalmm算法采用了类似于注意力机制的加权平均方式来融合各个模态经过模态融合网络后产生的特征，该模态融合的计算过程如@img:fusionway 所示。具体来说，evalmm算法首先对各模态的特征矩阵 $F in RR^(n times d)$  简单计算出了一个均值特征 $f in RR^(1 times d)$ ，之后将 $f$ 作为注意力计算的Q矩阵，$F^T$ 作为注意力计算的K矩阵，计算出权重矩阵 $W = f times F^T$，随后对权重矩阵作用softmax函数对权重归一化。最后通过 $W times V$ 计算得到最后融合的模态表征。通过注意力机制的方式，可以有效地关注到各模态特征集合中的每一个特征向量。

3. 分组计算 $I_d\C\o\r$ 指标。假设评估数据集共包含了 $N$ 个图像-标题对，那么经过第二个步骤后，将会得到 $N$ 个视觉-文本表征对。由于实际的评估数据集包含的图像-标题对的数量 $N$ 往往是非常大的，出于计算资源消耗的考量，将各模态的表征全部拼接后应用 $I_d\C\o\r$ 指标的计算相关性是不现实的，因为这会消耗大量的显存资源。因此evalmm算法将会根据可设置的batch_size值将原始的 $N$ 个视觉-文本表征对分为 $ceil(N / "batch_size")$ 组，针对每组的视觉-文本表征对，计算 $I_d\C\o\r$ 指标。这改成了评估所有视觉-文本表征对与计算资源消耗两者的折衷。本文建议将batch_size的取值设置为 $2^n$ ，且下界为128，这是因为过小的batch_size会因为每组样本量的过小导致评估结果的不可靠。


4. 区间统计，计算期望值。经过第三个步骤后，将会得到总计 $ceil(N / "batch_size")$ 个 $I_d\C\o\r$ 指标的值。统计这些值在步长为0.05的 $[0.00, 1.00]$ 区间上的频率分布（即直方图），并以频率比例模拟概率计算 $I_d\C\o\r$ 指标的期望值。若记区间 $[0.05i, 0.05(i+1)]$ 上的频次为 $f_i$ ，那么计算 $I_d\C\o\r$ 指标的期望值如@eqt:idcorgroupexp 所示。该期望值就是多模态大型语言模型模态对齐程度的评估结果。

$
  I_d\C\o\r = sum_(i=0)^(20)(0.05i + 1/2)f_i / N
$<idcorgroupexp>


#figure(
  image(
    "figures/evalmm.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [evalmm算法执行过程], // 英文图例
)<evalmm>


#figure(
  image(
    "figures/fusionway.png",
    width: 80%,
  ),
  kind: "image",
  supplement: [图],
  caption: [evalmm算法采取的模态融合方式计算过程], // 英文图例
)<fusionway>


#[
  #import "@preview/lovelace:0.2.0": *
  #algox(
    label-name: "evalalgo",
    caption: [evalmm评估算法伪代码],
    pseudocode(
      no-number,
      [#h(-1.25em) *input:* MLLM model, Eval Dataset $X$ and batch_size],
      no-number,
      [#h(-1.25em) *output:* multi-modality alignment to MLLM model ],
      [IMAGEREP $<-$ list[];],
      [TEXTREP $<-$ list[];],
      [*for* image, text in $X$ *do*],
      ind,
      [image_rep $<-$ Fusion(model.connect(image));],
      [text_rep $<-$ Fusion(model.connect(text));],
      [IMAGEREP.append(image_rep);],
      [TEXTREP.append(text_rep);],
      ded,
      [*end*],
      [IMAGEGROUP = Split(IMAGEREP, batch_size);],
      [TEXTGROUP = Split(TEXTREP, batch_size);],
      [IDCORVEC $<-$ list[];],
      [*for* imagerep, textrep in IMAGEGROUP, TEXTGROUP *do*],
      ind,
      [$I_d\c\o\r, p <- I_d\C\o\r"_Fun"("imagerep", "textrep")$;],
      [IDCORVEC.append(idcor);],
      ded,
      [*end*],

      [hist = HIST(IDCORVEC, 0, 1, 0.05);],
      [idcor_exp = Expectation(hist);],

      [*return* idcor_exp;],
    ),
  )
]

#v(1.5em)

== 实验评估
本小节从实验的角度去证明evalmm算法的有效性。实验主要对以LLaVA为代表的直接投影法的多模态大型语言模型进行了模态对齐程度的评估。具体而言，在本实验中，LLaVA模型的视觉编码器选取了开源预训练CLIP@clip 模型的视觉模型分支，即Vit-Large-Patch14-336，LLaVA模型的文本模型选取了Qwen2.5-3B-Instruct@bai2023qwentechnicalreport 模型，其拥有出色的指令跟随能力，LLaVA模型的模态融合网络选取了一个具有一个隐藏层的MLP神经网络。这种组合的LLaVA模型大致占用了7GB的显存。需要指出的是，由于LLaVA模型的模态融合网络是一个MLP神经网络，仅针对视觉特征投影到文本特征的语义空间中，因此evalmm算法中的特征融合步骤将对投影后的视觉特征进行特征融合，对文本模态每个token对应的大型语言模型嵌入向量embedding进行特征融合。

Haotian Liu等人@llava 在训练LLaVA模型时的训练策略分为两阶段，第一个阶段在LLaVA-CC3M-Pretrain-595K@llava 图像问答数据集上仅对模态融合网络MLP的参数进行预训练，目的是实现图文模态之间的对齐；第二个阶段在特定的下游任务训练集上对模态融合网络MLP和文本模型的参数进行指令微调，目的是让LLaVA模型能够在特定下游任务上表现出色。其中LLaVA-CC3M-Pretrain-595K数据集是Haotian Liu等人通过对CC3M数据集进行数据清洗后得到的一个图像问答数据集，清洗后的数据集在所描述的物品的语义分布上更为均匀，且滤除了过短和过长的回答。

本实验遵循这么一种训练策略，针对LLaVA模型首先进行了在第一阶段模态对齐的训练。具体而言，在LLaVA-CC3M-Pretrain-595K图像标题对数据集上进行了3个epoch的训练，一个批次的大小为32，学习率为2e-3，采用余弦学习率调度器调整学习率，在2张3090显卡的服务器上训练大致需要60个小时。在整个训练过程中，每训练2000步便保存一次模型参数。训练完成后，使用evalmm算法对保存的所有模型进行模态对齐程度的评估，选取的评估模态对齐程度的数据集仍旧是LLaVA-CC3M-Pretrain-595K（仅使用图像及问题）。模型的每个checkpoint应用evalmm算法评估得到的各指标数值分布图如@img:evalmmhist 所示。全部模型计算得到的模态对齐程度数据如@img:idcorchange 所示。

#figure(
  image(
    "figures/evalmm/evalhistshow.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [应用evalmm算法评估得到的各指标数值分布图], // 英文图例
)<evalmmhist>

#v(1.5em)


#figure(
  image(
    "figures/evalmm/idcorchange.png",
    width: 81%,
  ),
  kind: "image",
  supplement: [图],
  caption: [LLaVA stage1训练过程中的模态对齐程度变化], // 英文图例
)<idcorchange>

#v(1.5em)

应用evalmm算法评估LLaVA模型在第一阶段训练过程中的模态对齐程度变化符合预期。这是因为在第一阶段模态对齐的训练过程中，虽然仅训练MLP网络的参数，但是随着MLP网络的训练，经过MLP投影后的视觉特征将逐步对齐于文本特征的嵌入向量，因此模态对齐的程度将会逐渐上升，并有一种收敛的趋势。@img:idcorchange 中的模态对齐程度变化曲线符合了这一点，同时体现了上升及收敛的特征，且收敛于0.47，对于 $I_d\C\o\r$ 指标在衡量多模态表征时是一个较高的值。这表明evalmm算法是有效的。

== 多模态大型语言模型模态对齐程度对下游任务的影响
本小节是对evalmm评估算法的扩展应用。多模态大型语言模型的训练往往分为两个阶段（如LLaVA，BLIP-2等）：第一个阶段的目标是进行多模态间的对齐，第二个阶段的目标是进行下游任务的微调。本小节旨在探讨多模态大型语言模型在第一个阶段训练时达成的模态对齐程度对下游任务性能上的影响。

具体而言，本文以在4.3小节中设置的LLaVA模型为例，在第一阶段模态对齐的训练中，在LLaVA-CC3M-Pretrain-595K图像问答数据集上训练3个epoch，一个批次的大小为32，学习率为2e-3，采用余弦学习率调度器调整学习率，仅训练模态融合网络MLP，并且间隔2000步进行模型的保存，同时运用evalmm算法对保存的模型进行模态对齐程度的评估。在第二阶段的训练中，选取了ScienceQA@scienceqa 作为下游任务Benchmark，其是一个广泛使用的多模态科学问答数据集，通过选项的方式来选出正确的答案。对第一阶段中保存的所有模型进行3个epoch的ScienceQA训练集的微调，一个批次的大小为16，学习率为2e-3，采用余弦学习率调度器调整学习率。之后运用ScienceQA的测试集对模型进行评估，并记录其性能表现。此外，针对微调后的模型，也使用evalmm算法对其模态对齐程度进行评估。最后将第一阶段模态对齐训练过程中的模态对齐程度与第二阶段下游任务微调过程中的性能以及模态对齐程度进行对比分析。本实验中使用的evalmm评估数据集皆为LLaVA-CC3M-Pretrain-595K（仅使用图像及问题）。

实验结果如@img:sqaexp 和@img:idcorcompare 所示，在@img:sqaexp 中，蓝色的折线代表了全部问题的正确率，绿色的线代表非多模态问题的正确率，橙色的线代表多模态问题的正确率。在@img:idcorcompare 中，红色的线代表微调前模型的模态对齐程度，青色的线代表微调后模型的模态对齐程度。不难观察到，模型在第一阶段模态对齐的程度总体上与模型在下游任务上进行微调后的性能呈现一定的正相关。尤其是在16000、32000、50000步这些接近于一阶段每个epoch训练结束时的checkpoint附近表现出了较好的性能。


#figure(
  image(
    "figures/evalmm/acc&mmacc&nonmmacc.png",
    width: 100%,
  ),
  kind: "image",
  supplement: [图],
  caption: [stage1不同checkpoint模型在ScienceQA数据集微调后的性能], // 英文图例
)<sqaexp>

#figure(
  image(
    "figures/evalmm/idcorcompare.png",
    width: 80%,
  ),
  kind: "image",
  supplement: [图],
  caption: [LLaVA stage1训练过程中的模态对齐程度变化], // 英文图例
)<idcorcompare>

#v(4em)

== 本章小结
本章主要介绍了基于 $I_d\C\o\r$ 指标所设计的评估多模态大型语言模型模态对齐程度的算法evalmm。对其执行过程进行了详细的阐述。此外，应用evalmm算法对实际的多模态大型语言模型LLaVA在第一阶段模态对齐的训练过程中进行了评估，评估结果符合预期，表明evalmm算法是客观有效的。同时，evalmm算法的设计过程充分考虑了计算资源的消耗，是一个高效的评估算法。在本章的最后部分，还利用了evalmm算法对多模态大型语言模型模态对齐程度对下游任务的影响进行了探讨，实验结果表明模态对齐程度与下游任务性能之间存在一定的正相关性，这为多模态相关的研究提供了新的视角。


= 总结与展望
本章主要是对全文的研究内容以及所提出方法的总结，并且对实验方法还存在
着的一些未能解决的问题以及未来可能的解决方案进行展望。

== 总结
本文的研究内容主要是通过大量详实的实验条件，充分对比了 $I_d\C\o\r$ 指标与其他基线指标在捕获高维多模态表征之间复杂非线形关系的性能差异，最终验证了 $I_d\C\o\r$ 指标在多模态表征上的强大性能，证明了 $I_d\C\o\r$ 指标具有捕获多模态表征之间复杂非线性关系的能力，且在高维表征及多模态表征上都表现优异，同时对于存在一定噪声的数据时也具有良好的鲁棒性。之后，本文设计了一个评估多模态大型语言模型模态对齐程度的算法evalmm。evalmm算法是基于 $I_d\C\o\r$ 指标设计的，evalmm算法的设计过程充分考虑了计算资源的消耗，是一个高效的评估算法。evalmm算法的执行过程包括四个步骤：准备步骤、模态特征抽取与融合、分组计算 $I_d\C\o\r$ 指标、区间统计并计算期望值。并且本文也在实际的多模态大型语言模型LLaVA中验证了evalmm评估算法的有效性。同时本文还利用了evalmm评估算法探讨了多模态模型微调前的模态对齐程度与下游任务性能之间的关系，实验表明两者存在一定的正相关性。

本文的创新点可以分为以下几点：
1. 创新点一：本文通过设置大量详实的实验条件，充分对比了 $I_d\C\o\r$ 指标与其他基线指标在捕获高维多模态表征之间复杂非线形关系的性能差异。

2. 创新点二：本文基于 $I_d\C\o\r$ 指标设计了一个评估多模态大型语言模型模态对齐程度的算法evalmm。evalmm算法易于计算，在设计时充分考虑了计算资源的消耗，且在实际的多模态大型语言模型模态对齐程度评估中表现出了良好的性能。

3. 创新点三：本论文从多模态表征之间的非线性相关性出发，借助evalmm算法计算了多模态大型语言模型在第一个阶段训练时模态融合网络的模态对齐程度的实时变化，并且首次以具体评估数值的方式探讨了模态对齐程度与下游任务性能之间的关系。这将有助于从一个新的角度进一步理解多模态大语言模型如何整合不同模态间的信息。

== 展望
1. 本文虽然通过实证证明了evalmm评估算法的有效性，但是仅在以LLaVA为代表的直接投影法的多模态大型语言模型上进行了评估，选取的评估模态对齐程度的数据集也仅限于LLaVA-CC3M-Pretrain-595K图像问答对数据集。未来的工作可以选取更多的评估模态对齐的数据集以及在更多的多模态大型语言模型上进行评估（如BLIP-2），以期充分全面地验证evalmm算法的有效性。此外evalmm算法的设计中还应引入消融实验，替换不同的指标来分组计算表征之间的相关性系数来对比不同指标的性能，进而验证使用 $I_d\C\o\r$ 指标的有效性。在这之后本文将进行更多的探究。

2. 在evalmm算法中的特征融合步骤中，未进行消融实验来验证基于注意力机制的融合方式的有效性。未来的工作可以对比不同的特征融合方式（如简单平均，最大池化等）在evalmm算法中的表现，从而进行综合的对比，找寻最佳的特征融合方式并以此提升evalmm算法的性能，使其更为可信。

3. 探讨多模态大型语言模型在第一个阶段训练时达成的模态对齐程度对下游任务性能上的影响的实验中，选取的下游任务仅限于ScienceQA数据集。未来的工作可以选取更多的下游任务进行对比分析，例如VQAv2，TextVQA等，从而进行全面的探究。

// = 格式要求


// 正文各章节应拟标题，每章结束后应另起一页。标题要简明扼要，不应使用标点符号。各章、节、条的层次，可以按照“1……、1.1……、1.1.1……”标识，条以下具体款项的层次依次按照“1.1.1.1”或“（1）”、“①”等标识。各学院根据实际情况，可自行规定层次格式，但学院之内建议格式统一，以清晰无误为准@liu_survey_2024。

// 正文是毕业论文的主体和核心部分，不同学科专业和不同的选题可以有不同的写作方式。正文一般包括以下几个方面。

// == 引言或背景
// 引言是论文正文的开端，引言应包括毕业论文选题的背景、目的和意义；对国内外研究现状和相关领域中已有的研究成果的简要评述；介绍本项研究工作研究设想、研究方法或实验设计、理论依据或实验基础；涉及范围和预期结果等。要求言简意赅，注意不要与摘要雷同或成为摘要的注解。

// == 主体
// 论文主体是毕业论文的主要部分，必须言之成理，论据可靠，严格遵循本学科国际通行的学术规范。在写作上要注意结构合理、层次分明、重点突出，章节标题、公式图表符号必须规范统一。论文主体的内容根据不同学科有不同的特点，一般应包括以下几个方面：
// + 毕业设计（论文）总体方案或选题的论证；
// + 毕业设计（论文）各部分的设计实现，包括实验数据的获取、数据可行性及有效性的处理与分析、各部分的设计计算等；
// + 对研究内容及成果的客观阐述，包括理论依据、创新见解、创造性成果及其改进与实际应用价值等；
// + 论文主体的所有数据必须真实可靠，自然科学论文应推理正确、结论清晰；人文和社会学科的论文应把握论点正确、论证充分、论据可靠，恰当运用系统分析和比较研究的方法进行模型或方案设计，注重实证研究和案例分析，根据分析结果提出建议和改进措施等。

// == 结论
// 结论是毕业论文的总结，是整篇论文的归宿。应精炼、准确、完整。着重阐述自己的创造性成果及其在本研究领域中的意义、作用，还可进一步提出需要讨论的问题和建议。




// #conclusion[
//   结论是毕业论文的总结，是整篇论文的归宿。应精炼、准确、完整。着重阐述自己的创造性成果及其在本研究领域中的意义、作用，还可进一步提出需要讨论的问题和建议。
// ]

// 参考文献
#bib(bibfunc: bibliography("ref.bib", full: false)) // full: false 表示只显示已引用的文献，不显示未引用的文献；true 表示显示所有文献

// #show: appendix

// = 附录格式

// 论文附录依次用大写字母“附录A、附录B、附录C……”表示，附录内的分级序号可采用“附A1、附A1.1、附A1.1.1”等表示，图、表、公式均依此类推为“图A1、表A1、式（A1）”等。包含以下内容：

// （1）代码、图表、标准、手册等数据；

// （2）未发表过的一手文献；

// （3）公式推导与证明、调查表等；

// （4）辅助性教学工具或表格；

// （5）其他需要展示或说明的内容

// ……

// （标题黑体小二号，内容Times New Roman/宋体，小四号，行距20磅）

// == 测试1

// #figure(
//   image(
//     "figures/energy-distribution.png",
//     width: 70%,
//   ),
//   kind: "image",
//   supplement: [图],
//   caption: [Energy distribution along radial], // 英文图例
// )<image2>
// ......

// === 测试1.1

// #figure(
//   image(
//     "figures/energy-distribution.png",
//     width: 70%,
//   ),
//   kind: "image",
//   supplement: [图],
//   caption: [Energy distribution along radial], // 英文图例
// )<image3>

// = 测试2
// #figure(
//   image(
//     "figures/energy-distribution.png",
//     width: 70%,
//   ),
//   kind: "image",
//   supplement: [图],
//   caption: [Energy distribution along radial], // 英文图例
// )<image4>


// #algox(
//   label-name: "twoNNpytorch",
//   caption: [twoNN估算数据集本征维度算法PyTorch实现],
//   [
//     ```python
//     def twoNN(X,fraction=0.9,distances=False):
//       if not distances:
//           X=torch.cdist(X,X)
//       Y=torch.topk(X, 3, dim=1, largest=False)[0]
//       # clean data
//       k1 = Y[:,1]
//       k2 = Y[:,2]
//       #remove zeros and degeneracies (k1==k2)
//       old_k1=k1
//       k1 = k1[old_k1!=0]
//       k2 = k2[old_k1!=0]
//       old_k1=k1
//       k1 = k1[old_k1!=k2] 
//       k2 = k2[old_k1!=k2]
//       # n.of points to consider for the linear regression
//       npoints = int(np.floor(len(k1)*fraction))
//       # define mu and Femp
//       N = len(k1)
//       mu,_ = torch.sort(torch.divide(k2, k1).flatten())
//       Femp = (torch.arange(1,N+1,dtype=X.dtype))/N
//       # take logs (leave out the last element because 1-Femp is zero there)
//       x = torch.log(mu[:-1])[0:npoints]
//       y = -torch.log(1 - Femp[:-1])[0:npoints]
//       # regression, on gpu if available
//       y=y.to(x.device)
//       slope=torch.linalg.lstsq(x.unsqueeze(-1),y.unsqueeze(-1))
//       return slope.solution.squeeze()
//     ```
//   ],
// )

// ......

#acknowledgement(location: "上海大学")[
  时光荏苒，四年的大学生活即将画上句号。回首这四年的旅途，非常感谢在旅途中遇到的志同道合的朋友们，以及给予我无私帮助的老师们，我们一起经历了很多难忘的时光，克服了许多挑战，并达成了自身的成长，感谢你们的陪伴与支持，让我的旅途不再孤单并顺利完成冒险。

  在这里我想向我的全程导师段圣宇老师表示感谢，感谢段老师在我初入计算机学院时对于学习的规划与指导，让我明晰了未来的方向，可以说您是我进入计算机科学的引路人，在此真的很感谢您！此外我还想感谢郑宇老师，感谢郑老师在大三夏季实践的项目和推免研究生时对我的指导与帮助，让我收获了许多专业技能；同时我还想向许庆国老师致以诚挚的谢意，感谢许老师在联合大作业项目中带我引入了深度学习的世界以及为我攥写推荐信，您乐观的态度一直感染着我。同时也要感谢所有授课老师们，你们的严谨治学和认真负责为我打下了夯实的专业基础，让我在未来的科研及工作中有足够的能力去应对挑战。

  我很感谢我在大学期间能够进入算法竞赛集训队，这是一段难忘的经历，在这里我想特别感谢沈俊老师在竞赛上的指导与帮助。同时我也想感谢集训队的每一个同学们，感谢你们对竞赛内容的讲解和对我的帮助，你们的优秀不断激励着我继续进步。最后我还想特别感谢我的队友陈楠、陈欣宇，我们一起在课余时间集训解决难题，一起奔赴参加比赛，一起讨论比赛的题目共同成长。有你们在我从未害怕过比赛的挑战，我们一起做了很多有意思的事！

  在刚进入科研时，我的内心时常感到忐忑和迷茫，时常会因为一个问题而焦虑。但很感谢王含章老师、李崇新师兄、段涟师姐对我在科研路上的引领和帮助，感谢你们对我的热情接纳以及在我迷茫时的耐心指导和鼓励，并教授了我很多解决科研问题的思维方式，让我快速地入门科研。本文的主要工作在王含章老师的指导下完成，本论文实验部分的计算资源也由王老师提供，在此衷心感谢王老师对于本文的悉心指导和充分的支持。
  
  我还想感谢在大学期间结识的志同道合的好友——徐谟横、王志伟、陈欣宇、陈楠、张亚文、张泽毅、许贺、邓翔、黄溪雨、杨立天，能够与你们成为朋友一起成长是我大学生活中最快乐的回忆。我们一起上课学习，一起熬夜做项目，一起参加比赛，一起分享生活中的点滴，一起讨论梦想...... 我真的很幸运能够遇到你们，和你们的友谊是我大学生活中最珍贵的财富！最后，我还要感谢我的家人，感谢你们一直以来的支持与鼓励，你们的爱让我在追求梦想的路上从未孤单。无论未来的路有多么艰难，我都会努力前行。

  谨以此文，献给所有在这四年的旅途中给予我关心和支持的人们。

]

#under-cover()
